{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e6b931-c3c5-4150-a9e3-75093e28da76",
   "metadata": {},
   "source": [
    "# Cutout data augmentation\n",
    "\n",
    "In this notebook, we will reproduce the results of the paper\n",
    "\n",
    "> DeVries, T. and Taylor, G.W., 2017. Improved regularization of convolutional neural networks with Cutout. arXiv preprint [arXiv:1708.04552](https://arxiv.org/abs/1708.04552).\n",
    "\n",
    "We will use the author’s implementation of their technique, from <https://github.com/uoguelph-mlrg/Cutout>, which is licensed under an Educational Community License version 2.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72e29c4d-5f3b-46b6-941c-d5a81e0c855f",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "\n",
    "After working through this notebook, you should be able to:\n",
    "\n",
    "-   Describe how Cutout works as a regularization technique,\n",
    "-   Enumerate specific claims (both quantitative claims, qualitative claims, and claims about the underlying mechanism behind a result) from the Cutout paper,\n",
    "-   Execute experiments (following the given procedure) to try and validate each claim about Cutout data augmentation,\n",
    "-   Evaluate whether your own result matches quantitative claims in the Cutout paper (i.e. whether it is within the confidence intervals for each reported numeric result),\n",
    "-   Evaluate whether your own result validates qualitative claims in the Cutout paper,\n",
    "-   Evaluate whether your own results support the author’s claim about the underlying mechanism behind the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5d9dc-8efb-4656-9143-a2b058735e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ee802-9808-4b6e-9142-e0cae660a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: This notebook has been developed and tested for pytorch \n",
    "print(torch. __version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb6bc5cd-fd27-4b7b-abb6-744ebc18e68d",
   "metadata": {},
   "source": [
    "## Cutout as a regularization technique\n",
    "\n",
    "This Jupyter notebook is designed to illustrate the implementation and usage of the Cutout data augmentation technique in deep learning, specifically in the context of Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac59785-6bb3-44bb-8d40-1bba3a3e4651",
   "metadata": {},
   "source": [
    "Cutout is a regularization and data augmentation technique for convolutional neural networks (CNNs). It involves randomly masking out square regions of input during training. This helps to improve the robustness and overall performance of CNNs by encouraging the network to better utilize the full context of the image, rather than relying on the presence of a small set of specific visual features.\n",
    "\n",
    "Cutout is computationally efficient as it can be applied during data loading in parallel with the main training task. It can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The technique has been evaluated with state-of-the-art architectures on popular image recognition datasets such as CIFAR-10, CIFAR-100, and SVHN, often achieving state-of-the-art or near state-of-the-art results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37cee7b1-4144-414e-aa99-af22332c2a1b",
   "metadata": {},
   "source": [
    "In the following cells, we will see how Cutout works when applied to a sample image.\n",
    "\n",
    "<!-- To do: explain the code with reference to section 3.2. Implementation Details -->\n",
    "\n",
    "In the code provided above, we see a Python class named Cutout defined. This class is designed to apply the Cutout data augmentation technique to an image. Below is an explanation of the class and its methods:\n",
    "\n",
    "-   The Cutout class is initialized with two parameters:\n",
    "\n",
    "    -   `n_holes`: the number of patches to cut out of each image.\n",
    "    -   `length`: the length (in pixels) of each square patch.\n",
    "\n",
    "-   The `__call__` method implements the Cutout technique. This method takes as input a tensor `img` representing an image, and returns the same image with `n_holes` number of patches of dimension `length` x `length` cut out of it.\n",
    "\n",
    "Here’s a step-by-step explanation of what’s happening inside the `__call__` method:\n",
    "\n",
    "1.  The method first retrieves the height h and width w of the input image.\n",
    "\n",
    "2.  A mask is then initialized as a 2D numpy array of ones with the same dimensions as the input image.\n",
    "\n",
    "3.  The method then enters a loop which runs for n_holes iterations. In each iteration:\n",
    "\n",
    "    -   A pair of coordinates y and x are randomly selected within the height and width of the image.\n",
    "\n",
    "    -   The method then calculates the coordinates of a square patch around the (y, x) coordinate. The patch has a length of length pixels, and the method ensures that the patch doesn’t fall outside the image by using the np.clip function.\n",
    "\n",
    "    -   The corresponding area in the mask is set to zero.\n",
    "\n",
    "4.  The mask is then converted to a PyTorch tensor and expanded to the same number of channels as the input image.\n",
    "\n",
    "5.  Finally, the method applies the mask to the input image, effectively setting the pixels in the masked regions to zero, and returns the result.\n",
    "\n",
    "Remember to import necessary libraries like numpy (np) and PyTorch (torch) before running this class definition. The class Cutout can then be used as part of your data augmentation pipeline when training your models.\n",
    "\n",
    "The Cutout code we are using comes from this specific file in the original GitHub repository: \\[https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af80383-d51e-4dfa-a767-de3bafba0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: link to the file in the original repo that this comes from\n",
    "# Source Code from https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "708c0cb9-e726-479e-a475-cfd2a2c742dd",
   "metadata": {},
   "source": [
    "To see how it works, in the following cell, you will upload an image of your choice to this workspace:\n",
    "\n",
    "<!-- to do - add instructions for uploading image on Colab, or on Chameleon -->\n",
    "\n",
    "To see how Cutout works, let’s upload an image and apply Cutout to it. Follow these steps to upload an image in this Google Colab notebook:\n",
    "\n",
    "1.  Click on the folder icon in the left sidebar to open the ‘Files’ tab.\n",
    "2.  Click the ‘Upload to session storage’ button (the icon looks like a file with an up arrow).\n",
    "3.  Select the image file from your local machine that you want to upload.\n",
    "4.  Wait for the upload to finish. The uploaded file should now appear in the ‘Files’ tab. After the image is uploaded, we can use Python code to load it into our notebook and apply the Cutout augmentation\n",
    "\n",
    "If you are using Chameleon, here are the steps: <!-- to do - add instructions for uploading image on Chameleon --> 1. Click on the upload icon in the left sidebar. 2. Select the image file from your local machine that you want to upload. 3. Wait for the upload to finish. The uploaded file should now appear in the ‘Files’ tab. After the image is uploaded, we can use Python code to load it into our notebook and apply the Cutout augmentation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a85d8-0813-4f15-8cb9-2255220ae798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace 'sample.png' with the filename of your own image. \n",
    "# If your image is inside a directory, include the directory's name in the path.\n",
    "img = Image.open('/content/sample.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d94820c9-8b18-4663-9aa0-e458b1f638c9",
   "metadata": {},
   "source": [
    "Then, the following cell will display your image directly, without any data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d841b17-0af7-4c21-923e-925d0b54581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a PyTorch tensor\n",
    "img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img_tensor.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "864021c4-1e6d-4f72-9475-a6d7471b8d06",
   "metadata": {},
   "source": [
    "and the next cell will display your image with Cutout applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d685c4-ad14-4942-a700-7f471cbc232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cutout object\n",
    "Cutout = Cutout(n_holes=1, length=300)\n",
    "\n",
    "# Apply Cutout to the image\n",
    "img_tensor_Cutout = Cutout(img_tensor)\n",
    "\n",
    "# Convert the tensor back to an image for visualization\n",
    "img_Cutout = transforms.ToPILImage()(img_tensor_Cutout)\n",
    "\n",
    "# Display the image with Cutout applied\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img_tensor_Cutout.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73e5a49-49e1-4d29-932d-ae1300787c59",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "\n",
    "-   You can re-run the cell above several times to see how the occlusion is randomly placed in a different position each time.\n",
    "-   You can try changing the `length` parameter in the cell above, and re-running, to see how the size of the occlusion can change.\n",
    "-   You can try changing the `n_holes` parameter in the cell above, and re-running, to see how the number of occlusions can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59e9e7-2d9b-491c-9528-69bdb6aa87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO: Set the number of patches (\"holes\") to cut out of the image.\n",
    "n_holes = \n",
    "\n",
    "#TODO: Set the size (length of a side) of each patch.\n",
    "length = \n",
    "\n",
    "\n",
    "# Create a Cutout object\n",
    "Cutout = Cutout(n_holes, length)\n",
    "\n",
    "# Apply Cutout to the image\n",
    "img_tensor_Cutout = Cutout(img_tensor)\n",
    "\n",
    "# Convert the tensor back to an image for visualization\n",
    "img_Cutout = transforms.ToPILImage()(img_tensor_Cutout)\n",
    "\n",
    "# Display the image with Cutout applied\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img_tensor_Cutout.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4152d217-900f-49d4-9988-db6ff8daffb4",
   "metadata": {},
   "source": [
    "Cutout was introduced as an alternative to two closely related techniques:\n",
    "\n",
    "-   Data Augmentation for Images: Data augmentation is a strategy used to increase the diversity of the data available for training models, without actually collecting new data. For image data, this could include operations like rotation, scaling, cropping, flipping, and adding noise. The goal is to make the model more robust by allowing it to see more variations of the data.\n",
    "\n",
    "-   Dropout in Convolutional Neural Networks: Dropout is a regularization technique for reducing overfitting in neural networks. During training, some number of layer outputs are randomly ignored or “dropped out”. This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, dropout simulates ensembling a large number of neural networks with different architectures, which makes the model more robust.\n",
    "\n",
    "<!-- to do - expand on these -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbea9437-08cf-4f31-ae05-3012628c6db9",
   "metadata": {},
   "source": [
    "In the following code snippet, we demonstrate some “standard” data augmentation techniques commonly used in image preprocessing. These techniques include random horizontal flipping, random cropping, and color jittering (random variation in brightness, contrast, saturation, and hue). The augmented image is then displayed alongside the original image for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e47ac-2e1b-42c8-821a-b8b706b351ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do - show the same image with \"standard\" data augmentation techniques\n",
    "# discussed in the related work section of the paper\n",
    "\n",
    "# Import necessary libraries\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, ColorJitter\n",
    "\n",
    "# Define standard data augmentation techniques\n",
    "transforms = transforms.Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomCrop(size=(28, 28), padding=4),  # assuming input image is size 28x28\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "# Apply transformations to the image\n",
    "augmented_img = transforms(img)\n",
    "\n",
    "# Display the original and augmented image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(augmented_img)\n",
    "ax[1].set_title('Augmented Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7d4a41-ff7c-400a-950d-27b14b96f027",
   "metadata": {},
   "source": [
    "## Identifying claims from the Cutout paper\n",
    "\n",
    "To reproduce the results from the original Cutout paper, we will first need to identify the specific, falsifiable claims in that paper, by reading it very carefully. Then, we will design experiments to validate each claim.\n",
    "\n",
    "These claims may be quantitative (i.e. describe a specific numeric result), qualitative (i.e. describe a general characteristic of the result), or they may relate to the mechanism behind a result (i.e. describe *why* a particular result occurs).\n",
    "\n",
    "<!-- to do - go through the paper, quote little snippets and explain each claim and organize them -->\n",
    "\n",
    "### Qualitative Claims\n",
    "\n",
    "The authors of the Grad-CAM paper are discussing the intention behind their approach. When they say “We aimed to remove maximally activated features in order to encourage the network to consider less prominent features”, they are referring to the process of directing the neural network’s attention towards features that it might not normally focus on.\n",
    "\n",
    "When a convolutional neural network (CNN) processes an image, it learns to recognize certain features that are most useful for making correct predictions. These “useful” features are often referred to as “maximally activated” because the network’s activation functions output high values for these features. This means that these features are given a lot of importance when the network is making a decision.\n",
    "\n",
    "However, just because these features are the most prominent, doesn’t mean they are the only important ones. There can be other, less prominent features that are also useful for the task. By removing the maximally activated features, the authors are attempting to force the network to pay more attention to these less prominent features. This could potentially lead to a more robust model, as it won’t be overly reliant on a small set of features.\n",
    "\n",
    "So in essence, this claim is about trying to increase the diversity of the features that the network considers important, with the goal of improving the model’s ability to generalize from the training data to unseen data.\n",
    "\n",
    "### Quantitative Claims\n",
    "\n",
    "#### ResNet18\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**       | **CIFAR-10** | **CIFAR-100** |\n",
    "|-------------------|--------------|---------------|\n",
    "| ResNet18          | 4.72         | 22.46         |\n",
    "| ResNet18 + cutout | 3.99         | 21.96         |\n",
    "\n",
    "#### WideResNet\n",
    "\n",
    "WideResNet model implementation from https://github.com/xternalz/WideResNet-pytorch\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**         | **CIFAR-10** | **CIFAR-100** | **SVHN** |\n",
    "|---------------------|--------------|---------------|----------|\n",
    "| WideResNet          | 3.87         | 18.8          | 1.60     |\n",
    "| WideResNet + cutout | 3.08         | 18.41         | **1.30** |\n",
    "\n",
    "#### Shake-shake Regularization Network\n",
    "\n",
    "Shake-shake regularization model implementation from https://github.com/xgastaldi/shake-shake\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 3 runs)\n",
    "\n",
    "| **Network**          | **CIFAR-10** | **CIFAR-100** |\n",
    "|----------------------|--------------|---------------|\n",
    "| Shake-shake          | 2.86         | 15.58         |\n",
    "| Shake-shake + cutout | 2.56         | 15.20         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2beb1374-78ee-4b19-937e-901a349b8185",
   "metadata": {},
   "source": [
    "## Execute experiments to validate quantitative and qualitative claims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4983eddb-bb19-4625-bbc3-2bc9364cd8fe",
   "metadata": {},
   "source": [
    "### Implement Cutout on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3b273-6f82-4847-9170-da92ad76ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file names 'checkpoints' to save the weight of the models\n",
    "\n",
    "#If you use Jupyter Notebook\n",
    "if not os.path.exists('/content/checkpoints'):\n",
    "    os.makedirs('/content/checkpoints')\n",
    "\n",
    "#If you use Chameleon\n",
    "'''\n",
    "if not os.path.exists('/checkpoints'):\n",
    "    os.makedirs('/checkpoints')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590b9f6-a150-4903-a1aa-fa6e33d2a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #,\n",
    "    #Cutout(n_holes=1, length=16)  # Cutout applied here\n",
    "])\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013465d-878e-4631-a524-8c1df66c32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset with transformations applied\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c64c3-f364-4d2b-a5b9-ccd027ddbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images before Cutout\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c273262-0639-4ef8-b6ef-e66fe2591f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Cutout and show images after\n",
    "Cutout_images = torch.stack([Cutout(n_holes=1, length=16)(img) for img in images])\n",
    "imshow(torchvision.utils.make_grid(Cutout_images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e5a8d95-92d8-40fe-965f-0c8cdd2baa90",
   "metadata": {},
   "source": [
    "### 4. Methods and Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84d87983-c2b0-47f0-b55c-70e92682a89f",
   "metadata": {},
   "source": [
    "### ResNet Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8b9aa-afc4-4c6b-ac83-fbc6473f3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "# From https://github.com/uoguelph-mlrg/Cutout/blob/master/model/resnet.py\n",
    "\n",
    "'''ResNet18/34/50/101/152 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = conv3x3(planes, planes)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion*planes:\n",
    "      self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "  expansion = 4\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(Bottleneck, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion*planes:\n",
    "      self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),nn.BatchNorm2d(self.expansion*planes))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = F.relu(self.bn2(self.conv2(out)))\n",
    "    out = self.bn3(self.conv3(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self, block, num_blocks, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.in_planes = 64\n",
    "\n",
    "    self.conv1 = conv3x3(3,64)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    strides = [stride] + [1]*(num_blocks-1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "      layers.append(block(self.in_planes, planes, stride))\n",
    "      self.in_planes = planes * block.expansion\n",
    "      return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = self.layer4(out) # We'll need this output for Grad-CAM\n",
    "    self.activations = out  # Save the activations\n",
    "    out = F.avg_pool2d(out, 4)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.linear(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "  return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "  return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "  return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
    "\n",
    "def ResNet101(num_classes=10):\n",
    "  return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
    "\n",
    "def ResNet152(num_classes=10):\n",
    "  return ResNet(Bottleneck, [3,8,36,3], num_classes)\n",
    "\n",
    "def test_resnet():\n",
    "  net = ResNet50()\n",
    "  y = net(Variable(torch.randn(1,3,32,32)))\n",
    "  print(y.size())\n",
    "\n",
    "# test_resnet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa0d2a1f-df1a-42f2-90b7-43b1f0409521",
   "metadata": {},
   "source": [
    "### WideResNet Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79900242-feca-476e-974f-12e663f99787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WideResNet\n",
    "\n",
    "# From https://github.com/uoguelph-mlrg/Cutout/blob/master/model/wide_resnet.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d3519c-b333-4baf-ae52-4e9bd4088cff",
   "metadata": {},
   "source": [
    "The `CSVLogger` class logs training progress to a CSV file, with each row representing an epoch and columns representing metrics such as training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ac0bd-c134-4451-9cf1-a83d8579efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/uoguelph-mlrg/Cutout/blob/master/util/misc.py\n",
    "class CSVLogger():\n",
    "    def __init__(self, args, fieldnames, filename='log.csv'):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.csv_file = open(filename, 'w')\n",
    "\n",
    "        # Write model configuration at top of csv\n",
    "        writer = csv.writer(self.csv_file)\n",
    "        for arg in vars(args):\n",
    "            writer.writerow([arg, getattr(args, arg)])\n",
    "        writer.writerow([''])\n",
    "\n",
    "        self.writer = csv.DictWriter(self.csv_file, fieldnames=fieldnames)\n",
    "        self.writer.writeheader()\n",
    "\n",
    "        self.csv_file.flush()\n",
    "\n",
    "    def writerow(self, row):\n",
    "        self.writer.writerow(row)\n",
    "        self.csv_file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.csv_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b18ac3e-b816-4411-b117-72ac11d7ca43",
   "metadata": {},
   "source": [
    "### 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4604f9-1dca-423a-bb3f-e3758eb16ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def main(args):\n",
    "  args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "  cudnn.benchmark = True  # Should make training should go faster for large models\n",
    "\n",
    "\n",
    "  torch.manual_seed(args.seed)\n",
    "  if args.cuda:\n",
    "      torch.cuda.manual_seed(args.seed) \n",
    "\n",
    "  test_id = args.dataset + '_' + args.model\n",
    "\n",
    "  print(args)\n",
    "\n",
    "  # Image Preprocessing\n",
    "  if args.dataset == 'svhn':\n",
    "      normalize = transforms.Normalize(mean=[x / 255.0 for x in[109.9, 109.7, 113.8]],\n",
    "                                      std=[x / 255.0 for x in [50.1, 50.6, 50.8]])\n",
    "  else:\n",
    "      normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                      std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "  train_transform = transforms.Compose([])\n",
    "  if args.data_augmentation:\n",
    "      train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "      train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "  train_transform.transforms.append(transforms.ToTensor())\n",
    "  train_transform.transforms.append(normalize)\n",
    "  if args.Cutout:\n",
    "      train_transform.transforms.append(Cutout(n_holes=args.n_holes, length=args.length))\n",
    "\n",
    "\n",
    "  test_transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      normalize])\n",
    "\n",
    "  if args.dataset == 'cifar10':\n",
    "      num_classes = 10\n",
    "      train_dataset = datasets.CIFAR10(root='data/',\n",
    "                                      train=True,\n",
    "                                      transform=train_transform,\n",
    "                                      download=True)\n",
    "\n",
    "      test_dataset = datasets.CIFAR10(root='data/',\n",
    "                                      train=False,\n",
    "                                      transform=test_transform,\n",
    "                                      download=True)\n",
    "  elif args.dataset == 'cifar100':\n",
    "      num_classes = 100\n",
    "      train_dataset = datasets.CIFAR100(root='data/',\n",
    "                                        train=True,\n",
    "                                        transform=train_transform,\n",
    "                                        download=True)\n",
    "\n",
    "      test_dataset = datasets.CIFAR100(root='data/',\n",
    "                                      train=False,\n",
    "                                      transform=test_transform,\n",
    "                                      download=True)\n",
    "  elif args.dataset == 'svhn':\n",
    "      num_classes = 10\n",
    "      train_dataset = datasets.SVHN(root='data/',\n",
    "                                    split='train',\n",
    "                                    transform=train_transform,\n",
    "                                    download=True)\n",
    "\n",
    "      extra_dataset = datasets.SVHN(root='data/',\n",
    "                                    split='extra',\n",
    "                                    transform=train_transform,\n",
    "                                    download=True)\n",
    "\n",
    "      # Combine both training splits (https://arxiv.org/pdf/1605.07146.pdf)\n",
    "      data = np.concatenate([train_dataset.data, extra_dataset.data], axis=0)\n",
    "      labels = np.concatenate([train_dataset.labels, extra_dataset.labels], axis=0)\n",
    "      train_dataset.data = data\n",
    "      train_dataset.labels = labels\n",
    "\n",
    "      test_dataset = datasets.SVHN(root='data/',\n",
    "                                  split='test',\n",
    "                                  transform=test_transform,\n",
    "                                  download=True)\n",
    "\n",
    "  # Data Loader (Input Pipeline)\n",
    "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=2)\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=2)\n",
    "\n",
    "  if args.model == 'resnet18':\n",
    "      cnn = ResNet18(num_classes=num_classes)\n",
    "  elif args.model == 'wideresnet':\n",
    "      if args.dataset == 'svhn':\n",
    "          cnn = WideResNet(depth=16, num_classes=num_classes, widen_factor=8,\n",
    "                          dropRate=0.4)\n",
    "      else:\n",
    "          cnn = WideResNet(depth=28, num_classes=num_classes, widen_factor=10,\n",
    "                          dropRate=0.3)\n",
    "\n",
    "  cnn = cnn.cuda()\n",
    "  criterion = nn.CrossEntropyLoss().cuda()\n",
    "  cnn_optimizer = torch.optim.SGD(cnn.parameters(), lr=args.learning_rate,\n",
    "                                  momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "\n",
    "  if args.dataset == 'svhn':\n",
    "      scheduler = MultiStepLR(cnn_optimizer, milestones=[80, 120], gamma=0.1)\n",
    "  else:\n",
    "      scheduler = MultiStepLR(cnn_optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "\n",
    "  filename = 'logs/' + test_id + '.csv'\n",
    "  csv_logger = CSVLogger(args=args, fieldnames=['epoch', 'train_acc', 'test_acc'], filename=filename)\n",
    "\n",
    "\n",
    "  def test(loader):\n",
    "      cnn.eval()    # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "      correct = 0.\n",
    "      total = 0.\n",
    "      for images, labels in loader:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "          with torch.no_grad():\n",
    "              pred = cnn(images)\n",
    "\n",
    "          pred = torch.max(pred.data, 1)[1]\n",
    "          total += labels.size(0)\n",
    "          correct += (pred == labels).sum().item()\n",
    "\n",
    "      val_acc = correct / total\n",
    "      cnn.train()\n",
    "      return val_acc\n",
    "\n",
    "\n",
    "  for epoch in range(args.epochs):\n",
    "\n",
    "      xentropy_loss_avg = 0.\n",
    "      correct = 0.\n",
    "      total = 0.\n",
    "\n",
    "      progress_bar = tqdm(train_loader)\n",
    "      for i, (images, labels) in enumerate(progress_bar):\n",
    "          progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "          cnn.zero_grad()\n",
    "          pred = cnn(images)\n",
    "\n",
    "          xentropy_loss = criterion(pred, labels)\n",
    "          xentropy_loss.backward()\n",
    "          cnn_optimizer.step()\n",
    "\n",
    "          xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "          # Calculate running average of accuracy\n",
    "          pred = torch.max(pred.data, 1)[1]\n",
    "          total += labels.size(0)\n",
    "          correct += (pred == labels.data).sum().item()\n",
    "          accuracy = correct / total\n",
    "\n",
    "          progress_bar.set_postfix(\n",
    "              xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "              acc='%.3f' % accuracy)\n",
    "\n",
    "      test_acc = test(test_loader)\n",
    "      tqdm.write('test_acc: %.3f' % (test_acc))\n",
    "\n",
    "      #scheduler.step(epoch)  # Use this line for PyTorch <1.4\n",
    "       scheduler.step()     # Use this line for PyTorch >=1.4\n",
    "\n",
    "      row = {'epoch': str(epoch), 'train_acc': str(accuracy), 'test_acc': str(test_acc)}\n",
    "      csv_logger.writerow(row)\n",
    "\n",
    "  torch.save(cnn.state_dict(), 'checkpoints/' + test_id + '.pt')\n",
    "  csv_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e239236-4065-4cdd-bf6b-9b3e51302e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset = 'cifar10' #dataset_options = ['cifar10', 'cifar100', 'svhn']\n",
    "    model = 'resnet18' # model_options = ['resnet18', 'wideresnet']\n",
    "    batch_size = 128\n",
    "    epochs = 200\n",
    "    learning_rate = 0.1\n",
    "    data_augmentation = False\n",
    "    Cutout = False\n",
    "    n_holes = 1\n",
    "    length = 16\n",
    "    no_cuda = False #False means use Cuda GPU\n",
    "    seed = 1 #Default from the source code\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61662a34-4182-4629-937c-adb342e7120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 5\n",
    "\n",
    "\n",
    "for i in range(num_runs):\n",
    "    print(\"Experiment -\"+str(i)+\" out of \"+str(num_ruunns))\n",
    "    main(args)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "046b7b53-0ec2-44de-918d-040803ef6c71",
   "metadata": {},
   "source": [
    "## Evaluate your results for qualitative and quantitative claims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4956078b-1225-40c3-8c4e-0f7b23d81111",
   "metadata": {},
   "source": [
    "#### ResNet18\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**       | **CIFAR-10** | **CIFAR-100** |\n",
    "|-------------------|--------------|---------------|\n",
    "| ResNet18          | 4.72         | 22.46         |\n",
    "| ResNet18 + cutout | 3.99         | 21.96         |\n",
    "\n",
    "#### WideResNet\n",
    "\n",
    "WideResNet model implementation from https://github.com/xternalz/WideResNet-pytorch\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**         | **CIFAR-10** | **CIFAR-100** | **SVHN** |\n",
    "|---------------------|--------------|---------------|----------|\n",
    "| WideResNet          | 3.87         | 18.8          | 1.60     |\n",
    "| WideResNet + cutout | 3.08         | 18.41         | **1.30** |\n",
    "\n",
    "#### Shake-shake Regularization Network\n",
    "\n",
    "Shake-shake regularization model implementation from https://github.com/xgastaldi/shake-shake\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 3 runs)\n",
    "\n",
    "| **Network**          | **CIFAR-10** | **CIFAR-100** |\n",
    "|----------------------|--------------|---------------|\n",
    "| Shake-shake          | 2.86         | 15.58         |\n",
    "| Shake-shake + cutout | 2.56         | 15.20         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbeee23d-1de9-46c2-8917-2c67e4428c9c",
   "metadata": {},
   "source": [
    "## Execute experiments to validate the suggested mechanism\n",
    "\n",
    "### Implementing GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01724f7e-4871-4df6-acf5-4217e928b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.5):\n",
    "    # Resize the heatmap to match the size of the image\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Convert the heatmap to RGB format\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Apply the heatmap to the image\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay the heatmap on the image with the specified alpha\n",
    "    superimposed_img = heatmap * alpha + image\n",
    "\n",
    "    # Return the superimposed image\n",
    "    return superimposed_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e78359cf-c35d-457b-9a26-800e287d7316",
   "metadata": {},
   "source": [
    "### Implementing GradCam\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5ec72-a363-4f94-a70e-3cbe558dc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "def gradcam(model, image, class_idx):\n",
    "    # Zero out existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image.unsqueeze(0))\n",
    "\n",
    "    # Calculate the gradients of the target class score w.r.t. input image\n",
    "    output[0, class_idx].backward()\n",
    "\n",
    "    # Apply global average pooling to the gradients\n",
    "    pooled_gradients = F.adaptive_avg_pool2d(model.gradient, 1)\n",
    "\n",
    "    # Multiply the feature maps by the pooled gradients\n",
    "    for i in range(model.gradient.shape[1]):\n",
    "        model.gradient[:, i, :, :] *= pooled_gradients[0, i]\n",
    "\n",
    "    # Average the channel-wise multiplied feature maps along the channel dimension\n",
    "    gradcam_heatmap = torch.mean(model.gradient, dim=1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Apply ReLU to the heatmap\n",
    "    gradcam_heatmap = np.maximum(gradcam_heatmap, 0)\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    gradcam_heatmap = gradcam_heatmap / np.max(gradcam_heatmap)\n",
    "\n",
    "    # Resize the heatmap to match the original image\n",
    "    gradcam_heatmap = cv2.resize(gradcam_heatmap, (image.shape[1], image.shape[2]))\n",
    "\n",
    "    return gradcam_heatmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aa73a21-8a25-4a4d-a832-ed0e34ef92ad",
   "metadata": {},
   "source": [
    "## Evaluate your results for validating the suggested mechanism\n",
    "\n",
    "### Implementing GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7443bd6-6e19-49fe-9900-e70ef3383ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = ResNet18(num_classes=10)  # Initialize model architecture\n",
    "model.load_state_dict(torch.load('checkpoints/cifar10_resnet18.pt'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dca59-ce77-4ca8-8875-d2ef2e7bd100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
