{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. WideResNet"
   ],
   "id": "76076f1c-c257-4448-a2c9-c9ed179744ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WideResNet model implementation from https://github.com/xternalz/WideResNet-pytorch\n",
    "\n",
    "Note: for faster training, use Runtime \\> Change Runtime Type to run this notebook on a GPU."
   ],
   "id": "958d8774-7731-48b6-a5c5-a762205e44b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Cutout paper, the authors claim that:\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "In this section, we will evaluate these claims using a WideResNet model. For the WideResNet model, the specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs) and “+” indicates standard data augmentation (mirror + crop)\n",
    "\n",
    "| **Network**         | **CIFAR-10** | **CIFAR-10+** | **CIFAR-100** | **CIFAR-100+** | **SVHN**    |\n",
    "|-----------|------------|-------------|------------|-------------|------------|\n",
    "| WideResNet          | 6.97 ± 0.22  | 3.87 ± 0.08   | 26.06 ± 0.22  | 18.8 ± 0.08    | 1.60 ± 0.05 |\n",
    "| WideResNet + cutout | 5.54 ± 0.08  | 3.08 ± 0.16   | 23.94 ± 0.15  | 18.41 ± 0.27   | 1.30 ± 0.03 |\n",
    "\n",
    "In this table, the effectiveness of standard and cutout data augmentation techniques is evaluated using the WideResNet architecture on the CIFAR-10, CIFAR-100, and SVHN datasets. The “+”, as before, indicates the use of standard data augmentation (mirror and crop).\n",
    "\n",
    "In the results table provided in the original paper, we have included confidence intervals to provide a comprehensive view of the outcomes. It’s important to note that in our case, due to computational constraints, we have trained and evaluated the model only once, whereas the original authors conducted multiple runs. Consequently, while our numerical outcomes may not precisely match their averaged values, we are interested in assessing whether our results fall within the specified confidence intervals. This approach allows us to gauge the consistency and reliability of our findings with respect to the reported results.\n",
    "\n",
    "For CIFAR-10, utilizing the WideResNet model with standard augmentation significantly reduces the test error from 6.97% to 3.87%. When cutout augmentation is added, the error drops even further to 3.08%.\n",
    "\n",
    "A comparable trend is seen with the CIFAR-100 dataset. Standard augmentation reduces the WideResNet model’s test error from 26.06% to 18.8%. With the application of cutout augmentation, the error rate decreases slightly more to 18.41%.\n",
    "\n",
    "Lastly, the SVHN dataset shows the smallest error rates. With standard augmentation, the error is 1.60% which further reduces to 1.30% with the addition of cutout augmentation.\n",
    "\n",
    "These results demonstrate the robust effectiveness of both standard and cutout augmentation techniques in lowering test error rates across all tested datasets when used with the WideResNet model. As with the previous findings, the effect of augmentation appears to be influenced by the complexity of the dataset."
   ],
   "id": "8f6df3b2-95ed-44dc-a7f4-c9977930fb00"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ],
   "id": "cd7e07f9-30f8-4c04-8b9a-5da4905c238a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math"
   ],
   "id": "e7eee211-007c-40ce-a536-de4a028d81ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Cuda GPU availability and set seed number"
   ],
   "id": "78f5db25-2ebc-43db-a9b6-9c44a9a4fb48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "cudnn.benchmark = True  # Should make training should go faster for large models\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "id": "f04bdeb5-fc9a-46e1-bd72-45d9bc05d2a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, here’s a step-by-step how to connect with your google drive:\n",
    "\n",
    "1.  On the left sidebar of the Colab notebook interface, you will see a folder icon with the Google Drive logo. Click on this folder icon to open the file explorer.\n",
    "\n",
    "2.  If you haven’t connected your Google Drive to Colab yet, it will prompt you to do so. Click the “Mount Drive” button to connect your Google Drive to Colab.\n",
    "\n",
    "3.  Once your Google Drive is mounted, you can use the file explorer to navigate to the file you want to open. Click on the folders to explore the contents of your Google Drive.\n",
    "\n",
    "4.  When you find the file you want to open, click the three dots next to the name of the file in the file explorer. From the options that appear, choose “Copy path.” This action will copy the full path of the file to your clipboard. Paste the copy path into the ‘current_path’ below."
   ],
   "id": "28137713-504b-4de1-a3a1-ceadfb1c293f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path =\"./\""
   ],
   "id": "ae26e047-1c33-40cb-8ffd-284661a66b58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is used for creating a directory named ‘checkpoints’. This directory will be used to store the weights of our models, which are crucial for both preserving our progress during model training and for future use of the trained models.\n",
    "\n",
    "Creating such a directory and regularly saving model weights is a good practice in machine learning, as it ensures that you can resume your work from where you left off, should the training process be interrupted."
   ],
   "id": "d5a8772f-66b3-4daf-b56a-175a8141b15e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file names 'checkpoints' to save the weight of the models\n",
    "if not os.path.exists(current_path + 'checkpoints'):\n",
    "    os.makedirs(current_path + 'checkpoints')"
   ],
   "id": "ee29a9af-7bc5-44c6-8f76-c29ee34ff372"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementation Code"
   ],
   "id": "8236354b-09c2-468d-8ade-481f38619845"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 WideResNet Code"
   ],
   "id": "82daa9ec-501b-4f0d-b851-345d05fa1975"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/uoguelph-mlrg/Cutout/blob/master/model/wide_resnet.py\n",
    "\n",
    "class BasicBlockWide(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlockWide, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) // 6\n",
    "        block = BasicBlockWide\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "72e7fb1e-0149-479d-a882-43aa6a7f4463"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Model Evaluate Test Code\n",
    "\n",
    "This function evaluates the performance of the model on a given data loader (loader). It sets the model to evaluation mode (eval), calculates the accuracy on the dataset, and returns the validation accuracy. It then switches the model back to training mode (train) before returning the validation accuracy."
   ],
   "id": "2d042d68-5f1a-4f02-b88f-050be2f34f35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, cnn):\n",
    "    cnn.eval()    # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for images, labels in loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = cnn(images)\n",
    "\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "    val_acc = correct / total\n",
    "    cnn.train()\n",
    "    return val_acc"
   ],
   "id": "2e091a39-bc2a-4748-8f51-214e081323cb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training WideResNet in CIFAR-10"
   ],
   "id": "8708a96a-173e-4576-b71d-07f8140b01e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Training WideResNet in CF10 without Cutout"
   ],
   "id": "7a9b593c-8be1-47a5-93cf-d32348925d05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments section, we will evaluate these claims using a WideResNet model in CIFAR-10 without or with cutout.\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**         | **CIFAR-10** |\n",
    "|---------------------|--------------|\n",
    "| WideResNet          | 6.97 ± 0.22  |\n",
    "| WideResNet + cutout | 5.54 ± 0.08  |"
   ],
   "id": "7e2b8769-9d6f-42cb-bbb7-5500d86f72b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10"
   ],
   "id": "2ccf3a32-adaa-4108-b67e-f79ab17b74ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10 = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar10.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "122cb5d8-3203-4459-8e2a-98a303aab333"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10"
   ],
   "id": "2560334e-528e-4e79-9e99-b839092a6c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "ec52b391-2075-4661-a756-3d1c533e2eda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-10 as Dataloader"
   ],
   "id": "06c6d88f-ed93-4fa2-86fb-9a12e461445e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10 = 128\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(dataset=train_dataset_cifar10,\n",
    "                                           batch_size=batch_size_cifar10,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "db82b2ce-df58-4900-a8a8-9012538a418d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-10"
   ],
   "id": "2eea287c-bebf-4678-9d49-4c85cb3d88a0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "da2c36a7-d145-4a0a-8f09-22c7d5f955f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar10 = \"wideresnet_cifar10\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "wideresnet_cifar10 = WideResNet(depth=28, num_classes=num_classes_cifar10, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar10 = wideresnet_cifar10.cuda()\n",
    "learning_rate_wideresnet_cifar10 = 0.1\n",
    "criterion_wideresnet_cifar10 = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar10 = torch.optim.SGD(wideresnet_cifar10.parameters(), lr=learning_rate_wideresnet_cifar10,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar10 = MultiStepLR(cnn_optimizer_wideresnet_cifar10, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "195da471-061e-412d-a055-6af22a7a1625"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown} Training WideResNet for CIFAR-10"
   ],
   "id": "ae03afc3-1479-4803-8719-61a0f278f462"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "d274d504-f6fc-498f-8741-d07217ace5bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar10.zero_grad()\n",
    "        pred = wideresnet_cifar10(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar10(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar10.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_accr_wideresnet_cifar10 = test(test_loader_cifar10, wideresnet_cifar10)\n",
    "    tqdm.write('test_acc: %.3f' % (test_accr_wideresnet_cifar10))\n",
    "\n",
    "    scheduler_wideresnet_cifar10.step()    \n",
    "\n",
    "    \n",
    "torch.save(wideresnet_cifar10.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar10 + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar10 = (1 - test(test_loader_cifar10, wideresnet_cifar10))*100\n",
    "print('Test error rates (%%) on  WideResNet without Cutout for Test CIFAR-10 Dataset: %.3f' % (final_test_acc_wideresnet_cifar10))"
   ],
   "id": "f40db576-36cb-4de0-bd60-5b0ca7f2d1fc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Training WideResNet in CF10 with Cutout"
   ],
   "id": "0ea72f6b-e9c0-424d-a4de-1885b9926b22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutout Code"
   ],
   "id": "2ad031f0-0f14-46c0-bef6-7895eb84411c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to do Cutout and it is the same code we introduced earlier in the section about Cutout"
   ],
   "id": "c982a9e1-dce5-4779-8d0d-2212efacf516"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Code from https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ],
   "id": "7734c999-e8d2-4f50-ad00-b6152fb245d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10 with Cutout"
   ],
   "id": "983ec333-5f4d-48d8-90d3-8b7aa6ebb01b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_cutout = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar10_cutout.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_cutout.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar10 = 1\n",
    "length_cifar10 = 16\n",
    "train_transform_cifar10_cutout.transforms.append(Cutout(n_holes=n_holes_cifar10, length=length_cifar10))\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "7667c6db-bc72-4b26-b1a3-2d30e966ff9c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10 with Cutout"
   ],
   "id": "3ff09680-e992-4c2d-93e4-7170a664e8e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_cutout = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_cutout,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "cdb9716e-461d-451f-88a9-c461ca688490"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-10 with Cutout as Dataloader"
   ],
   "id": "77734ffd-a388-4e08-b99b-85fe57670039"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_cutout = 128\n",
    "train_loader_cifar10_cutout = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_cutout,\n",
    "                                           batch_size=batch_size_cifar10_cutout,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_cutout,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "e53f4c98-ce12-4bcf-b25c-32a3358dbe0e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-10 with Cutout"
   ],
   "id": "a4e37a46-b614-4657-b5a3-4b591c241eb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "c354f65f-5845-4be3-b91d-67926b0bd7f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar10_cutout = \"wideresnet_cifar10_cutout\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "wideresnet_cifar10_cutout = WideResNet(depth=28, num_classes=num_classes_cifar10, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar10_cutout = wideresnet_cifar10_cutout.cuda()\n",
    "learning_rate_wideresnet_cifar10_cutout = 0.1\n",
    "criterion_wideresnet_cifar10_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar10_cutout = torch.optim.SGD(wideresnet_cifar10_cutout.parameters(), lr=learning_rate_wideresnet_cifar10_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar10_cutout = MultiStepLR(cnn_optimizer_wideresnet_cifar10_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "2f452917-b47c-40b6-b027-8c34d99039a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-10 with Cutout"
   ],
   "id": "4b2c256d-4c87-4523-babd-c096fa21bcd7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "c4279b3f-af17-499b-b014-6446594108ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_cutout)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar10_cutout.zero_grad()\n",
    "        pred = wideresnet_cifar10_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar10_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar10_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar10 = test(test_loader_cifar10,wideresnet_cifar10_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar10))\n",
    "    scheduler_wideresnet_cifar10_cutout.step()     \n",
    "torch.save(wideresnet_cifar10_cutout.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar10_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar10_cutout = (1 - test(test_loader_cifar10,wideresnet_cifar10_cutout))*100\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_cutout))"
   ],
   "id": "bc5349a9-c384-4c3e-bf98-99c5efdd4147"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Training WideResNet in CF10 with Data Augmentation"
   ],
   "id": "34a7e365-d4e6-47c1-b2df-b6fd6b72c5cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments section, we will evaluate these claims using a WideResNet model in CIFAR-10 with Data Augmentation and also without or with cutout.\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs) and “+” indicates standard data augmentation (mirror+ crop)\n",
    "\n",
    "| **Network**         | **CIFAR-10+** |\n",
    "|---------------------|---------------|\n",
    "| WideResNet          | 3.87 ± 0.08   |\n",
    "| WideResNet + cutout | 3.08 ± 0.16   |"
   ],
   "id": "35acd892-028d-4312-9606-8dda54499790"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10 with Data Augmentation"
   ],
   "id": "a1940a3a-00b5-48d3-9acc-1654b4f12ad2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_da = transforms.Compose([])\n",
    "train_transform_cifar10_da.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar10_da.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar10_da.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_da.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "3061d9b5-8ad5-4eab-8b58-7d1fa084e821"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10 with Data Augmentation"
   ],
   "id": "c94ccbfd-1248-4c91-bd20-bc7824261960"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_da = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_da,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "1e975ca4-e1c9-4871-91cb-1b59ec8165ab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-10 with Data Augmentation as Dataloader"
   ],
   "id": "fd92cdc6-0868-44d1-a136-17a2ff09cf35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_da = 128\n",
    "train_loader_cifar10_da = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_da,\n",
    "                                           batch_size=batch_size_cifar10_da,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_da,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "75f5558c-6165-4b6a-9d70-5fc8d4dc401f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-10 with Data Augmentation"
   ],
   "id": "042c7472-2921-4df9-9f64-eb24c0e37e3c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "2f02d80d-4d7c-4548-bffc-9cb98dfc0067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar10_da = \"wideresnet_cifar10_da\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "wideresnet_cifar10_da = WideResNet(depth=28, num_classes=num_classes_cifar10, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar10_da = wideresnet_cifar10_da.cuda()\n",
    "learning_rate_wideresnet_cifar10_da = 0.1\n",
    "criterion_wideresnet_cifar10_da = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar10_da = torch.optim.SGD(wideresnet_cifar10_da.parameters(), lr=learning_rate_wideresnet_cifar10_da,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar10_da = MultiStepLR(cnn_optimizer_wideresnet_cifar10_da, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "15f1520c-e592-4c6c-a5b1-bdfcfc3499ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-10 with Data Augmentation"
   ],
   "id": "0f533489-e960-4895-84dc-a8e021bf680e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "e79bf673-7ebf-46f4-ac2d-aa825c020ad6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_da)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar10_da.zero_grad()\n",
    "        pred = wideresnet_cifar10_da(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar10_da(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar10_da.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_wideresnet_cifar10_da = test(test_loader_cifar10,wideresnet_cifar10_da)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_wideresnet_cifar10_da))\n",
    "    scheduler_wideresnet_cifar10_da.step()     \n",
    "torch.save(wideresnet_cifar10_da.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar10_da + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar10_da = (1 - test(test_loader_cifar10,wideresnet_cifar10_da))*100\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_da))"
   ],
   "id": "30fb54c0-4b32-4ad4-8cca-0c8c93ce571e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Training WideResNet in CF10 with Data Augmentation and Cutout"
   ],
   "id": "46362f2e-23ec-4fe5-b2ed-8ab9d141d4d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10 with Data Augmentation and Cutout"
   ],
   "id": "6673bceb-9ae2-489d-830e-47514aceacf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_da_co = transforms.Compose([])\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_da_co.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar10_da_co = 1\n",
    "length_cifar10_da_co = 16\n",
    "train_transform_cifar10_da_co.transforms.append(Cutout(n_holes=n_holes_cifar10_da_co, length=length_cifar10_da_co))\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "bafb12e6-2d10-4973-aa56-0d27e54d2ac2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10 with Data Augmentation and Cutout"
   ],
   "id": "34998fa4-22ac-4c55-b0e2-683fd7b3fc58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_da_co = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_da_co,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "8aecdd57-29ef-492f-813a-70cadd385a7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-10 with Data Augmentation and Cutout as Dataloader"
   ],
   "id": "584d6ffb-6547-4221-9f60-e3ebb331a019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_da_co = 128\n",
    "train_loader_cifar10_da_co = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_da_co,\n",
    "                                           batch_size=batch_size_cifar10_da_co,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_da_co,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "89bbc43c-bfb6-4c82-9392-951580a0b0a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-10 with Data Augmentation and Cutout as Dataloader"
   ],
   "id": "fe8a6e45-97d2-4d87-8d5c-632927f0ff2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "e17e65ca-86f8-4f15-95d0-0e7d49b79bd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar10_da_cutout = \"wideresnet_cifar10_da_cutout\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "wideresnet_cifar10_da_cutout = WideResNet(depth=28, num_classes=num_classes_cifar10, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar10_da_cutout = wideresnet_cifar10_da_cutout.cuda()\n",
    "learning_rate_cifar10_da_cutout = 0.1\n",
    "criterion_cifar10_da_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_cifar10_da_cutout = torch.optim.SGD(wideresnet_cifar10_da_cutout.parameters(), lr=learning_rate_cifar10_da_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_cifar10_da_cutout = MultiStepLR(cnn_optimizer_cifar10_da_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "b010cee6-a02f-4e8a-8f57-47a7be72e271"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-10 with Data Augmentation and Cutout"
   ],
   "id": "b56c180a-e3c4-4fc5-82c4-2f533ce69a41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "85fea512-4f65-43ff-9030-a7c53f55ea04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_da_co)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar10_da_cutout.zero_grad()\n",
    "        pred = wideresnet_cifar10_da_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_cifar10_da_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_cifar10_da_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar10_da_cutout = test(test_loader_cifar10,wideresnet_cifar10_da_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar10_da_cutout))\n",
    "    scheduler_cifar10_da_cutout.step()     \n",
    "torch.save(wideresnet_cifar10_da_cutout.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar10_da_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar10_da_cutout = (1 - test(test_loader_cifar10,wideresnet_cifar10_da_cutout))*100\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation and  Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_da_cutout))"
   ],
   "id": "5f91d76d-cdf3-48dd-9b38-066355c33da0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test error rates (%%) on  WideResNet without Cutout for Test CIFAR-10 Dataset: %.3f' % (final_test_acc_wideresnet_cifar10))\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_cutout))\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_da))\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation and  Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar10_da_cutout))"
   ],
   "id": "4d2a665f-1bfa-4783-b585-9dd085ac3e30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training WideResNet in CIFAR-100"
   ],
   "id": "a5f58007-ffe4-43a2-825f-0673ae9a46ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Training WideResNet in CF100 without Cutout"
   ],
   "id": "bd7ddae5-f09d-4de5-b47f-35da261b2fcb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments section, we will evaluate these claims using a WideResNet model in CIFAR-100 without or with cutout.\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**         | **CIFAR-100** |\n",
    "|---------------------|---------------|\n",
    "| WideResNet          | 26.06 ± 0.22  |\n",
    "| WideResNet + cutout | 23.94 ± 0.15  |"
   ],
   "id": "5c013fb3-3ec6-48d4-9944-2fcfd1b63b9a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "00a1b93f-714a-4e12-a88c-4b02ec921131"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100 = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar100.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "84f203fe-ba22-471b-85f1-2100bd592d2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "40e4b4de-662e-447b-bba2-1a6bd36a9f6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100 = datasets.CIFAR100(root=current_path +  'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "ec1a2721-f2c7-4ee3-af31-bc12140b7c9f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-100 as Dataloader"
   ],
   "id": "02df04b6-0a6e-4361-86e2-5d7ad1680ef4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100 = 128\n",
    "train_loader_cifar100 = torch.utils.data.DataLoader(dataset=train_dataset_cifar100,\n",
    "                                           batch_size=batch_size_cifar100,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "03947501-aa5a-4b1a-a7b9-951aea5f2295"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-100"
   ],
   "id": "1741e448-a49f-4778-b87f-dd152362f935"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "203e7812-0e87-45b3-94db-506235906cc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar100 = \"wideresnet_cifar100\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "wideresnet_cifar100 = WideResNet(depth=28, num_classes=num_classes_cifar100, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar100 = wideresnet_cifar100.cuda()\n",
    "learning_rate_wideresnet_cifar100 = 0.1\n",
    "criterion_wideresnet_cifar100 = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar100 = torch.optim.SGD(wideresnet_cifar100.parameters(), lr=learning_rate_wideresnet_cifar100,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar100 = MultiStepLR(cnn_optimizer_wideresnet_cifar100, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "3391777d-d7a4-48c0-ae07-c38d42402f94"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-100"
   ],
   "id": "a13e689d-4056-4bf7-a46a-07e57604c80e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "9e1eba50-5a76-4796-aad3-538bc90a4a1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar100.zero_grad()\n",
    "        pred = wideresnet_cifar100(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar100(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar100.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_accr_wideresnet_cifar100 = test(test_loader_cifar100, wideresnet_cifar100)\n",
    "    tqdm.write('test_acc: %.3f' % (test_accr_wideresnet_cifar100))\n",
    "\n",
    "    scheduler_wideresnet_cifar100.step()    \n",
    "\n",
    "    \n",
    "torch.save(wideresnet_cifar100.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar100 + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar100 = (1 - test(test_loader_cifar100, wideresnet_cifar100))*100\n",
    "print('Test error rates (%%) on  WideResNet without Cutout for Test CIFAR-100 Dataset: %.3f' % (final_test_acc_wideresnet_cifar100))"
   ],
   "id": "ec12bf89-1f81-4e99-adda-2cbe7b6ed991"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Training WideResNet in CF100 with Cutout"
   ],
   "id": "57ca56ff-706e-4594-a2ae-5362994babae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "4e9b7fef-ffac-4fa0-837d-5568ae57e3bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_cutout = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar100_cutout.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_cutout.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar100 = 1\n",
    "length_cifar100 = 8\n",
    "train_transform_cifar100_cutout.transforms.append(Cutout(n_holes=n_holes_cifar100, length=length_cifar100))\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "43fb47f8-3c82-49a0-8b00-9f2ae6472203"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-0"
   ],
   "id": "1a72fa93-2daa-44d1-9edf-711c4fa89b9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_cutout = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_cutout,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "9138f623-5fe5-445e-ad2b-71b62a2d4bcd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-100 with Cutout as Dataloader"
   ],
   "id": "69ed23ac-2348-4cf7-ac66-18842eda5f58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_cutout = 128\n",
    "train_loader_cifar100_cutout = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_cutout,\n",
    "                                           batch_size=batch_size_cifar100_cutout,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_cutout,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "f0399203-64a7-4f9a-86bc-022dcffa348c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-100 with Cutout"
   ],
   "id": "2d19f074-a58a-4fa0-83d4-20af5a8b720d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "71cc7e48-91ac-4b11-a1ef-7e771d9fd796"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar100_cutout = \"wideresnet_cifar100_cutout\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "wideresnet_cifar100_cutout = WideResNet(depth=28, num_classes=num_classes_cifar100, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar100_cutout = wideresnet_cifar100_cutout.cuda()\n",
    "learning_rate_wideresnet_cifar100_cutout = 0.1\n",
    "criterion_wideresnet_cifar100_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar100_cutout = torch.optim.SGD(wideresnet_cifar100_cutout.parameters(), lr=learning_rate_wideresnet_cifar100_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar100_cutout = MultiStepLR(cnn_optimizer_wideresnet_cifar100_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "46262be3-73ef-4799-9510-94c4663caa0c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-100 with Cutout"
   ],
   "id": "16d613bb-a6fe-4fb7-81ed-df2cdf630e4b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "61f674fc-be3d-40e7-ae4c-0f9e8359a1dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_cutout)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar100_cutout.zero_grad()\n",
    "        pred = wideresnet_cifar100_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar100_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar100_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar100 = test(test_loader_cifar100,wideresnet_cifar100_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar100))\n",
    "    scheduler_wideresnet_cifar100_cutout.step()     \n",
    "torch.save(wideresnet_cifar100_cutout.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar100_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar100_cutout = (1 - test(test_loader_cifar100,wideresnet_cifar100_cutout))*100\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_cutout))"
   ],
   "id": "a57037b3-8ffa-4f05-88b0-b0e1144b2402"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Training WideResNet in CF100 with Data Augmentation"
   ],
   "id": "800f7dd3-7bd3-447d-a252-bcf41b95f441"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments section, we will evaluate these claims using a WideResNet model in CIFAR-100 with Data Augmentation and also without or with cutout.\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs) and “+” indicates standard data augmentation (mirror+ crop)\n",
    "\n",
    "| **Network**         | **CIFAR-100+** |\n",
    "|---------------------|----------------|\n",
    "| WideResNet          | 18.8 ± 0.08    |\n",
    "| WideResNet + cutout | 18.41 ± 0.27   |"
   ],
   "id": "c00620e8-9e47-4a42-aa69-e0f136c05155"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "476274ef-6fa6-4264-ac3c-9e38b3ed69ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_da = transforms.Compose([])\n",
    "train_transform_cifar100_da.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar100_da.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar100_da.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_da.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "a949ad65-4467-4865-8bb2-e09323c7ebb8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "540bd2b9-37a9-45e3-b6f6-f731efcd2f8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_da = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_da,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "0d8097b5-2ff2-41f3-b2bc-31f0e4ec17e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-100 with Data Augmentation as Dataloader"
   ],
   "id": "5f4cf2cd-5958-44a9-b3e7-f9702322e824"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_da = 128\n",
    "train_loader_cifar100_da = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_da,\n",
    "                                           batch_size=batch_size_cifar100_da,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_da,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "66c1c444-fa08-492a-833c-0318501514c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-100 with Data Augmentation"
   ],
   "id": "c8023698-44e3-46d1-a598-8d912f5bf2c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "def4cf57-3d44-4ce5-b7ff-29448cd0e9d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar100_da = \"wideresnet_cifar100_da\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "wideresnet_cifar100_da = WideResNet(depth=28, num_classes=num_classes_cifar100, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "\n",
    "wideresnet_cifar100_da = wideresnet_cifar100_da.cuda()\n",
    "learning_rate_wideresnet_cifar100_da = 0.1\n",
    "criterion_wideresnet_cifar100_da = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_cifar100_da = torch.optim.SGD(wideresnet_cifar100_da.parameters(), lr=learning_rate_wideresnet_cifar100_da,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_cifar100_da = MultiStepLR(cnn_optimizer_wideresnet_cifar100_da, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "a325fd47-ff68-44d7-8a6d-47ed9ba152f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-100 with Data Augmentation"
   ],
   "id": "5afb9f8f-fe21-41cd-a3bb-6fd6f9723451"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "76fd516e-2c8f-45c1-8ea7-b6423234f985"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_da)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar100_da.zero_grad()\n",
    "        pred = wideresnet_cifar100_da(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_cifar100_da(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_cifar100_da.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_wideresnet_cifar100_da = test(test_loader_cifar100,wideresnet_cifar100_da)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_wideresnet_cifar100_da))\n",
    "    scheduler_wideresnet_cifar100_da.step()     \n",
    "torch.save(wideresnet_cifar100_da.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar100_da + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar100_da = (1 - test(test_loader_cifar100,wideresnet_cifar100_da))*100\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_da))"
   ],
   "id": "eca7500c-de43-49ab-a399-9261088f723f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. Training WideResNet in CF100 with Data Augmentation with Cutout"
   ],
   "id": "fc7378d9-3f86-40f9-8195-f2660178b40f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "31164393-d19f-40b9-8f87-3589b40ae7c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], \n",
    "std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_da_co = transforms.Compose([])\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_da_co.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar100_da_co = 1\n",
    "length_cifar100_da_co = 8\n",
    "train_transform_cifar100_da_co.transforms.append(Cutout(n_holes=n_holes_cifar100_da_co, length=length_cifar100_da_co))\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "705713d9-2642-495b-a7bb-24f54711fda7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "667c5c5c-dde0-45e5-ac41-7a5f123c28a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_da_co = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_da_co,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "7cc59ba4-b8ad-4b29-8d2c-4699e46e4566"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset CIFAR-100 with Cutout and Data Augmentation as Dataloader"
   ],
   "id": "8f0f6018-57a2-4157-8b02-62dfaafed88d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_da_co = 128\n",
    "train_loader_cifar100_da_co = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_da_co,\n",
    "                                           batch_size=batch_size_cifar100_da_co,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_da_co,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "a4091ff4-d6cc-42c4-9ec1-48af3698158b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for CIFAR-100 with Data Augmentation and Cutout"
   ],
   "id": "ab8a0c9e-ac4d-419b-a8c0-afd5fc15571a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "2b8f7424-ac4c-46bf-bba4-32f9c21f8ad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_cifar100_da_cutout = \"wideresnet_cifar100_da_cutout\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "wideresnet_cifar100_da_cutout = WideResNet(depth=28, num_classes=num_classes_cifar100, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "\n",
    "wideresnet_cifar100_da_cutout = wideresnet_cifar100_da_cutout.cuda()\n",
    "learning_rate_cifar100_da_cutout = 0.1\n",
    "criterion_cifar100_da_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_cifar100_da_cutout = torch.optim.SGD(wideresnet_cifar100_da_cutout.parameters(), lr=learning_rate_cifar100_da_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_cifar100_da_cutout = MultiStepLR(cnn_optimizer_cifar100_da_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "133ef703-7870-4ee7-b868-62c63ee04f11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for CIFAR-100 with Data Augmentation and Cutout"
   ],
   "id": "16afc09d-1b9c-4bc5-9890-a824abbbb392"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "5c1f188b-3027-42db-9e22-114c9090c22c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_da_co)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_cifar100_da_cutout.zero_grad()\n",
    "        pred = wideresnet_cifar100_da_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_cifar100_da_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_cifar100_da_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar100_da_cutout = test(test_loader_cifar100,wideresnet_cifar100_da_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar100_da_cutout))\n",
    "    scheduler_cifar100_da_cutout.step()     \n",
    "torch.save(wideresnet_cifar100_da_cutout.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_cifar100_da_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_cifar100_da_cutout = (1 - test(test_loader_cifar100,wideresnet_cifar100_da_cutout))*100\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation and  Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_da_cutout))"
   ],
   "id": "d958b200-9aea-4f37-bef3-7be29124a7a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test error rates (%%) on  WideResNet without Cutout for Test CIFAR-100 Dataset: %.3f' % (final_test_acc_wideresnet_cifar100))\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_cutout))\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_da))\n",
    "print('Test error rates (%%) on  WideResNet using Data Augmentation and  Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_wideresnet_cifar100_da_cutout))"
   ],
   "id": "bfc16f51-218a-4bc7-98ad-ae46a9b23f53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training WideResNet in SVHN"
   ],
   "id": "7aeeccf1-dd53-4b0a-adb3-f949a28bce73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments section, we will evaluate these claims using a WideResNet model in SVHN with Data Augmentation and also without or with cutout.\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "The specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs)\n",
    "\n",
    "| **Network**         | **SVHN**    |\n",
    "|---------------------|-------------|\n",
    "| WideResNet          | 1.60 ± 0.05 |\n",
    "| WideResNet + cutout | 1.30 ± 0.03 |"
   ],
   "id": "a6d3701f-f275-4169-8b55-e74867b10e18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Training WideResNet in SVHN without Cutout"
   ],
   "id": "d250d49e-01a9-43dc-acb4-07da74f09078"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for SVHN"
   ],
   "id": "8b8c77c0-f801-4949-8cd5-db98df673977"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_svhn = transforms.Normalize(mean=[x / 255.0 for x in[109.9, 109.7, 113.8]],\n",
    "std=[x / 255.0 for x in [50.1, 50.6, 50.8]])\n",
    "\n",
    "train_transform_svhn = transforms.Compose([])\n",
    "\n",
    "train_transform_svhn.transforms.append(transforms.ToTensor())\n",
    "train_transform_svhn.transforms.append(normalize_image_svhn)\n",
    "\n",
    "\n",
    "\n",
    "test_transform_svhn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_svhn])"
   ],
   "id": "6caf3245-fc63-4968-8bac-1b422aa77b1b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of SVHN"
   ],
   "id": "dc246499-f246-48a3-8250-f7a382938dd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_svhn = datasets.SVHN(root=current_path + 'data/',\n",
    "                                    split='train',\n",
    "                                    transform=train_transform_svhn,\n",
    "                                    download=True)\n",
    "\n",
    "extra_dataset_svhn = datasets.SVHN(root=current_path + 'data/',\n",
    "                                    split='extra',\n",
    "                                    transform=train_transform_svhn,\n",
    "                                    download=True)\n",
    "\n",
    "# Combine both training splits (https://arxiv.org/pdf/1605.07146.pdf)\n",
    "data_svhn = np.concatenate([train_dataset_svhn.data, extra_dataset_svhn.data], axis=0)\n",
    "labels_svhn = np.concatenate([train_dataset_svhn.labels, extra_dataset_svhn.labels], axis=0)\n",
    "train_dataset_svhn.data = data_svhn\n",
    "train_dataset_svhn.labels = labels_svhn\n",
    "\n",
    "test_dataset_svhn = datasets.SVHN(root=current_path + 'data/',\n",
    "                                  split='test',\n",
    "                                  transform=test_transform_svhn,\n",
    "                                  download=True)"
   ],
   "id": "aaefbce3-c777-4a0d-a4fe-a45790ca94f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset SVHN as Dataloader"
   ],
   "id": "44ddedb4-550c-417a-928f-7e492caec800"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_svhn = 128\n",
    "train_loader_svhn = torch.utils.data.DataLoader(dataset=train_dataset_svhn,\n",
    "                                           batch_size=batch_size_svhn,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_svhn = torch.utils.data.DataLoader(dataset=test_dataset_svhn,\n",
    "                                          batch_size=batch_size_svhn,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "9c61679d-df8b-4a50-a05d-dcd6146a8423"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model of WideResNet for SVHN"
   ],
   "id": "6753dd3f-4ef5-4e88-a46d-2ced8da2ee5e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "a3ee6cd1-1549-4eb9-b3d8-42db4eee880d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_svhn = \"wideresnet_svhn\"\n",
    "\n",
    "num_classes_svhn = 10\n",
    "wideresnet_svhn = WideResNet(depth=16, num_classes=num_classes_svhn, widen_factor=8,dropRate=0.4)\n",
    "\n",
    "\n",
    "wideresnet_svhn = wideresnet_svhn.cuda()\n",
    "learning_rate_wideresnet_svhn = 0.01\n",
    "criterion_wideresnet_svhn = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_svhn = torch.optim.SGD(wideresnet_svhn.parameters(), lr=learning_rate_wideresnet_svhn,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_svhn = MultiStepLR(cnn_optimizer_wideresnet_svhn, milestones=[80, 120], gamma=0.1)"
   ],
   "id": "8408a331-0659-471a-bb55-a2a42e06c5d8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for SVHN without Cutout"
   ],
   "id": "60248bc3-d90a-41c8-bdb6-2742432657d7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "43673d25-9ca6-4146-a9c9-411205aa3237"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 160\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_svhn)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_svhn.zero_grad()\n",
    "        pred = wideresnet_svhn(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_svhn(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_svhn.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_accr_wideresnet_svhn = test(test_loader_svhn, wideresnet_svhn)\n",
    "    tqdm.write('test_acc: %.3f' % (test_accr_wideresnet_svhn))\n",
    "\n",
    "    scheduler_wideresnet_svhn.step()     \n",
    "\n",
    "    \n",
    "torch.save(wideresnet_svhn.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_svhn + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_svhn = (1 - test(test_loader_svhn, wideresnet_svhn))*100\n",
    "print('Test error rates (%%) on  WideResNet without Cutout for Test SVHN Dataset: %.3f' % (final_test_acc_wideresnet_svhn))"
   ],
   "id": "2a4321ab-7096-4ae9-9ce6-0e1097494d58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Training WideResNet in SVHN with Cutout"
   ],
   "id": "3da92d0a-d375-4e13-8bdf-9ed09e123431"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for SVHN"
   ],
   "id": "45d8154a-55bd-497c-8875-4f6ecd1b7645"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_svhn = transforms.Normalize(mean=[x / 255.0 for x in[109.9, 109.7, 113.8]], \n",
    "std=[x / 255.0 for x in [50.1, 50.6, 50.8]])\n",
    "\n",
    "train_transform_svhn_cutout = transforms.Compose([])\n",
    "\n",
    "train_transform_svhn_cutout.transforms.append(transforms.ToTensor())\n",
    "train_transform_svhn_cutout.transforms.append(normalize_image_svhn)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_svhn = 1\n",
    "length_svhn = 20\n",
    "train_transform_svhn_cutout.transforms.append(Cutout(n_holes=n_holes_svhn, length=length_svhn))\n",
    "\n",
    "\n",
    "test_transform_svhn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_svhn])"
   ],
   "id": "06ea3e29-5c4a-41db-89d0-1980bd675bbb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of SVHN"
   ],
   "id": "105d7611-c015-4079-b42a-375dba998b78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_svhn_cutout = datasets.SVHN(root=current_path + 'data/',\n",
    "                                    split='train',\n",
    "                                    transform=train_transform_svhn_cutout,\n",
    "                                    download=True)\n",
    "\n",
    "extra_dataset_svhn_cutout = datasets.SVHN(root=current_path + 'data/',\n",
    "                                    split='extra',\n",
    "                                    transform=train_transform_svhn_cutout,\n",
    "                                    download=True)\n",
    "\n",
    "# Combine both training splits (https://arxiv.org/pdf/1605.07146.pdf)\n",
    "data_svhn_cutout = np.concatenate([train_dataset_svhn_cutout.data, extra_dataset_svhn_cutout.data], axis=0)\n",
    "labels_svhn_cutout = np.concatenate([train_dataset_svhn_cutout.labels, extra_dataset_svhn_cutout.labels], axis=0)\n",
    "train_dataset_svhn_cutout.data = data_svhn_cutout\n",
    "train_dataset_svhn_cutout.labels = labels_svhn_cutout\n",
    "\n",
    "test_dataset_svhn = datasets.SVHN(root=current_path + 'data/',\n",
    "                                  split='test',\n",
    "                                  transform=test_transform_svhn,\n",
    "                                  download=True)"
   ],
   "id": "6ceb76a4-9557-4db6-817a-ee00f87d76c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset SVHN with Cutout as Dataloader"
   ],
   "id": "7a572bbc-2481-4f53-ad06-22709ce95916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_svhn_cutout = 128\n",
    "train_loader_svhn_cutout = torch.utils.data.DataLoader(dataset=train_dataset_svhn_cutout,\n",
    "                                           batch_size=batch_size_svhn_cutout,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_svhn = torch.utils.data.DataLoader(dataset=test_dataset_svhn,\n",
    "                                          batch_size=batch_size_svhn_cutout,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "1fadfbb3-18e2-4836-a2bb-1f251e8814ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model WideResNet for SVHN with Cutout"
   ],
   "id": "990e1daf-d9e9-4dcc-9fee-3197a7041d15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "ccf2cc39-0c83-4d61-9d3c-89dcbaa2cd32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_wideresnet_svhn_cutout = \"wideresnet_svhn_cutout\"\n",
    "\n",
    "num_classes_svhn = 10\n",
    "wideresnet_svhn_cutout = WideResNet(depth=16, num_classes=num_classes_svhn, widen_factor=8,dropRate=0.4)\n",
    "\n",
    "\n",
    "wideresnet_svhn_cutout = wideresnet_svhn_cutout.cuda()\n",
    "learning_rate_wideresnet_svhn_cutout = 0.01\n",
    "criterion_wideresnet_svhn_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_wideresnet_svhn_cutout = torch.optim.SGD(wideresnet_svhn_cutout.parameters(), lr=learning_rate_wideresnet_svhn_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_wideresnet_svhn_cutout = MultiStepLR(cnn_optimizer_wideresnet_svhn_cutout, milestones=[80, 120], gamma=0.1)"
   ],
   "id": "c2e5a691-2e10-46f1-b082-0caf1da4f108"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training WideResNet for SVHN with Cutout"
   ],
   "id": "ec73761b-d56e-4bea-8114-e97308e42c81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "87aede0b-49a0-4456-8b2b-136ec8e46aa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 160\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_svhn_cutout)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        wideresnet_svhn_cutout.zero_grad()\n",
    "        pred = wideresnet_svhn_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_wideresnet_svhn_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_wideresnet_svhn_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_svhn = test(test_loader_svhn,wideresnet_svhn_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_svhn))\n",
    "    scheduler_wideresnet_svhn_cutout.step()     \n",
    "torch.save(wideresnet_svhn_cutout.state_dict(), current_path + 'checkpoints/' + file_name_wideresnet_svhn_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_wideresnet_svhn_cutout = (1 - test(test_loader_svhn,wideresnet_svhn_cutout))*100\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for SVHN Test Dataset: %.3f' % (final_test_acc_wideresnet_svhn_cutout))"
   ],
   "id": "af03b610-90f3-4452-a111-42d2fe9d0602"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test error rates (%%) on  WideResNet without Cutout for Test SVHN Dataset: %.3f' % (final_test_acc_wideresnet_svhn))\n",
    "print('Test error rates (%%) on  WideResNet using Cutout for SVHN Test Dataset: %.3f' % (final_test_acc_wideresnet_svhn_cutout))"
   ],
   "id": "c18ead16-18d1-41c0-bedb-559dd7f82320"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
