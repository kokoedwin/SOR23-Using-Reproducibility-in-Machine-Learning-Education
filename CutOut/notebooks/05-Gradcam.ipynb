{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.3.2. Compare the Qualitative Claims usinng Grad-CAM\n",
    "\n",
    "###### What is Grad-CAM?\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique that provides visual explanations for decisions made by Convolutional Neural Network (CNN) models. It uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.\n",
    "\n",
    "Grad-CAM is not limited to a specific architecture, it can be applied to a wide range of CNN models without any changes to their existing structure or requiring re-training. It’s also class-discriminative, allowing it to effectively manage multi-label scenarios.\n",
    "\n",
    "By visualizing the model’s focus areas with Grad-CAM, we can assess how effectively Cutout is encouraging the model to use a broader range of features. For example, if a model trained with Cutout still primarily focuses on a single region, that might suggest the Cutout squares are too small, or not numerous enough. Conversely, if the focus areas are well spread across the image, it would confirm that Cutout is indeed pushing the model to generalize better.\n",
    "\n",
    "If you want to understand more about Grad-CAM? Check this paper (https://arxiv.org/abs/1610.02391)"
   ],
   "id": "0df40439-9215-49e5-9faf-27da29907edd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "        # Register hooks for Grad-CAM\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.layer4.register_forward_hook(self._store_activations_hook)\n",
    "        self.layer4.register_backward_hook(self._store_gradients_hook)\n",
    "\n",
    "    def _store_activations_hook(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def _store_gradients_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out) \n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ],
   "id": "7da370fd-ccfc-4485-b94d-3ec6bf103b1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet18(num_classes=10)\n",
    "model.load_state_dict(torch.load(\"checkpoints/cifar10_resnet18.pt\"))\n",
    "model.eval()\n",
    "\n",
    "model_co = ResNet18(num_classes=10)\n",
    "model_co.load_state_dict(torch.load(\"checkpoints/cifar10_resnet18_Cutout.pt\"))\n",
    "model_co.eval()"
   ],
   "id": "013d5f0b-13f8-400f-8591-5e1a79e4a9ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to see the result from the testloader of CIFAR-10 dataset"
   ],
   "id": "d48e3506-c9ea-4c60-ab8c-daa59c19a051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transform_dataset = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_dataset)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n"
   ],
   "id": "8e4dccc5-db9f-44e0-8c24-79fcf758c22e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_classes = [\n",
    "    \"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\",\n",
    "    \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"\n",
    "]"
   ],
   "id": "78a0f0a5-a15a-4d79-a6b0-6c7ac532d68d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the testloader\n",
    "images, labels = next(iter(testloader))\n",
    "input_tensor = images  # As your batch_size is 1, you will have a single image here\n",
    "\n",
    "# Forward pass\n",
    "model.zero_grad()\n",
    "output = model(input_tensor)\n",
    "\n",
    "model_co.zero_grad()\n",
    "output_co = model_co(input_tensor)\n",
    "\n",
    "# Get the index of the max log-probability\n",
    "target = output.argmax(1)\n",
    "output.max().backward()\n",
    "\n",
    "target_co = output_co.argmax(1)\n",
    "output_co.max().backward()\n",
    "\n",
    "# Map the predicted class indices to the class labels\n",
    "predicted_class = cifar_classes[target.item()]\n",
    "predicted_class_co = cifar_classes[target_co.item()]\n",
    "\n",
    "\n",
    "# Get the gradients and activations\n",
    "gradients = model.gradients.detach().cpu()\n",
    "activations = model.activations.detach().cpu()\n",
    "\n",
    "gradients_co = model_co.gradients.detach().cpu()\n",
    "activations_co = model_co.activations.detach().cpu()\n",
    "\n",
    "\n",
    "# Calculate the weights\n",
    "weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "weights_co = gradients_co.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "# Calculate the weighted sum of activations (Grad-CAM)\n",
    "cam = (weights * activations).sum(dim=1, keepdim=True)\n",
    "cam = F.relu(cam)  # apply ReLU to the heatmap\n",
    "cam = F.interpolate(cam, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "cam = cam.squeeze().numpy()\n",
    "\n",
    "cam_co = (weights_co * activations_co).sum(dim=1, keepdim=True)\n",
    "cam_co = F.relu(cam_co)  # apply ReLU to the heatmap\n",
    "cam_co = F.interpolate(cam_co, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "cam_co = cam_co.squeeze().numpy()\n",
    "\n",
    "\n",
    "# Normalize the heatmap\n",
    "cam -= cam.min()\n",
    "cam /= cam.max()\n",
    "\n",
    "cam_co -= cam_co.min()\n",
    "cam_co /= cam_co.max()\n",
    "\n",
    "# Since the images from the dataloader are normalized, you have to denormalize them before plotting\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "img = images.squeeze().detach().cpu() * std[..., None, None] + mean[..., None, None]\n",
    "img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Superimpose the heatmap onto the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "superimposed_img = heatmap * 0.4 + img * 255\n",
    "\n",
    "heatmap_co = cv2.applyColorMap(np.uint8(255 * cam_co), cv2.COLORMAP_JET)\n",
    "heatmap_co = cv2.cvtColor(heatmap_co, cv2.COLOR_BGR2RGB)\n",
    "superimposed_img_co = heatmap_co * 0.4 + img * 255\n",
    "\n",
    "class_label = str(labels.item())\n",
    "\n",
    "# Display the original image and the Grad-CAM\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image (Class: ' + cifar_classes[int(class_label)] + ')')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(superimposed_img / 255)\n",
    "ax[1].set_title('Grad-CAM: ' + predicted_class)\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(superimposed_img_co / 255)\n",
    "ax[2].set_title('Grad-CAM with Cutout:'+  predicted_class_co)\n",
    "ax[2].axis('off')\n",
    "plt.show()\n"
   ],
   "id": "f645b26f-51be-47ae-abe5-69c25ebbc776"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "9ada0156-c2a9-4ea9-a436-2ae4ba25181b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try to load your image, preprocess it and convert it into a PyTorch tensor. Choose an image that is in the CIFAR-10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks). The preprocessing steps should be the same as the ones you used for training your model. Let’s say you have an image `image.jpg`:"
   ],
   "id": "bdcca4db-34b0-4f4c-8e8c-7e46b9693e8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the image\n",
    "image_path = \"image.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define the transformations: resize, to tensor, normalize (replace the mean and std with values you used for training)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # add batch dimension.  C,H,W => B,C,H,W\n"
   ],
   "id": "4dd739fa-cd61-4773-8ade-9fee3c1cc324"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Grad-CAM"
   ],
   "id": "04c6a4a4-2dd5-40e8-b514-d2c88f6eafdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "model.zero_grad()\n",
    "output = model(input_tensor)\n",
    "\n",
    "model_co.zero_grad()\n",
    "output_co = model_co(input_tensor)\n",
    "\n",
    "# Get the index of the max log-probability\n",
    "target = output.argmax(1)\n",
    "output.max().backward()\n",
    "\n",
    "target_co  = output_co .argmax(1)\n",
    "output_co .max().backward()\n",
    "\n",
    "# Get the gradients and activations\n",
    "gradients = model.gradients.detach().cpu()\n",
    "activations = model.activations.detach().cpu()\n",
    "\n",
    "gradients_co  = model_co.gradients.detach().cpu()\n",
    "activations_co  = model_co.activations.detach().cpu()\n",
    "\n",
    "# Calculate the weights\n",
    "weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "weights_co = gradients_co.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "# Calculate the weighted sum of activations (Grad-CAM)\n",
    "cam = (weights * activations).sum(dim=1, keepdim=True)\n",
    "cam = F.relu(cam)  # apply ReLU to the heatmap\n",
    "cam = F.interpolate(cam, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "cam = cam.squeeze().numpy()\n",
    "\n",
    "cam_co = (weights_co * activations_co).sum(dim=1, keepdim=True)\n",
    "cam_co = F.relu(cam_co)  # apply ReLU to the heatmap\n",
    "cam_co = F.interpolate(cam_co, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "cam_co = cam_co.squeeze().numpy()\n",
    "\n",
    "# Normalize the heatmap\n",
    "cam -= cam.min()\n",
    "cam /= cam.max()\n",
    "\n",
    "cam_co -= cam_co.min()\n",
    "cam_co /= cam_co.max()\n"
   ],
   "id": "a784e33b-040a-4103-a83d-0a895454b726"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the image and the Grad-CAM heatmap"
   ],
   "id": "467d9ae9-f6b4-488c-a57c-3c509efd701c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Load the original image\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.resize(img, (32, 32))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Superimpose the heatmap onto the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "# Superimpose the heatmap onto the original image with cutout\n",
    "heatmap_co = cv2.applyColorMap(np.uint8(255 * cam_co), cv2.COLORMAP_JET)\n",
    "heatmap_co = cv2.cvtColor(heatmap_co, cv2.COLOR_BGR2RGB)\n",
    "superimposed_img_co = heatmap_co * 0.4 + img\n",
    "\n",
    "# Display the original image and the Grad-CAM\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(superimposed_img / 255)\n",
    "ax[1].set_title('Grad-CAM')\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(superimposed_img_co / 255)\n",
    "ax[2].set_title('Grad-CAM with Cutout')\n",
    "ax[2].axis(\"off\")\n",
    "plt.show()\n"
   ],
   "id": "84bc99a4-c906-4499-8d94-458cf06b4d3d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
