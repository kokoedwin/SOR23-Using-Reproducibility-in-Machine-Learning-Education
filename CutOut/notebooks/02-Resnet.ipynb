{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. ResNet"
   ],
   "id": "236544a5-757b-4bf1-ac41-d7bccb07ebb8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for faster training, use Runtime \\> Change Runtime Type to run this notebook on a GPU."
   ],
   "id": "fa776671-5de0-4031-9d7f-584b93a6be77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Cutout paper, the authors claim that:\n",
    "\n",
    "1.  Cutout improves the robustness and overall performance of convolutional neural networks.\n",
    "2.  Cutout can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance.\n",
    "\n",
    "In this section, we will evaluate these claims using a ResNet model. For the ResNet model, the specific quantitative claims are given in the following table:\n",
    "\n",
    "Test error (%, flip/translation augmentation, mean/std normalization, mean of 5 runs) and “+” indicates standard data augmentation (mirror + crop)\n",
    "\n",
    "| **Network**       | **CIFAR-10** | **CIFAR-10+** | **CIFAR-100** | **CIFAR-100+** |\n",
    "|-------------|--------------|----------------|--------------|----------------|\n",
    "| ResNet18          | 10.63        | 4.72          | 36.68         | 22.46          |\n",
    "| ResNet18 + cutout | 9.31         | 3.99          | 34.98         | 21.96          |\n",
    "\n",
    "The provided table displays the results of experiments conducted on the CIFAR-10 and CIFAR-100 datasets using the ResNet18 architecture, revealing the impact of standard and cutout data augmentation techniques. The “CIFAR-10+” and “CIFAR-100+” labels indicate the use of standard data augmentation, which involves mirror and crop techniques.\n",
    "\n",
    "With the use of standard data augmentation on CIFAR-10, the ResNet18 model’s test error is significantly reduced from 14.04% to 5.72%. Further enhancement is achieved when cutout augmentation is applied, bringing down the error to 4.86%. A similar pattern is observed in the case of the CIFAR-100 dataset, where standard augmentation reduces the ResNet18 model’s test error from 40.13% to 24.36%. Upon applying cutout augmentation, a slight further reduction in error to 23.9% is noted.\n",
    "\n",
    "These findings emphasize the efficacy of both standard and cutout data augmentation techniques in enhancing the model’s performance, evidenced by the reduction in test error rates on both CIFAR-10 and CIFAR-100 datasets. The results also highlight that the impact of data augmentation can vary based on the complexity of the dataset, illustrated by the differing rates of error reduction between CIFAR-10 and CIFAR-100."
   ],
   "id": "6525544e-bb92-4556-ae27-9b6501ef0fd0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ],
   "id": "faf90656-2eef-478e-b876-499367ce0417"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "id": "7fe2b0a2-fec9-49c3-9f3f-63e63ce613bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Cuda GPU availability and set seed number"
   ],
   "id": "491ca9e5-3fdc-4886-af49-d5342e7b2906"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "cudnn.benchmark = True  # Should make training should go faster for large models\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "id": "08a392fb-b30a-467b-b76a-625773280d4f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, here’s a step-by-step how to connect with your google drive:\n",
    "\n",
    "1.  On the left sidebar of the Colab notebook interface, you will see a folder icon with the Google Drive logo. Click on this folder icon to open the file explorer.\n",
    "\n",
    "2.  If you haven’t connected your Google Drive to Colab yet, it will prompt you to do so. Click the “Mount Drive” button to connect your Google Drive to Colab.\n",
    "\n",
    "3.  Once your Google Drive is mounted, you can use the file explorer to navigate to the file you want to open. Click on the folders to explore the contents of your Google Drive.\n",
    "\n",
    "4.  When you find the file you want to open, click the three dots next to the name of the file in the file explorer. From the options that appear, choose “Copy path.” This action will copy the full path of the file to your clipboard. Paste the copy path into the ‘current_path’ below."
   ],
   "id": "5e604605-b697-494a-8df8-639295831532"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path =\"./\""
   ],
   "id": "3cf598f1-1646-4309-b57e-c07fd8bee916"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is used for creating a directory named ‘checkpoints’. This directory will be used to store the weights of our models, which are crucial for both preserving our progress during model training and for future use of the trained models.\n",
    "\n",
    "Creating such a directory and regularly saving model weights is a good practice in machine learning, as it ensures that you can resume your work from where you left off, should the training process be interrupted."
   ],
   "id": "28e12fcf-5714-43bc-a2fc-a0cfb43f2d77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file names 'checkpoints' to save the weight of the models\n",
    "if not os.path.exists(current_path + 'checkpoints'):\n",
    "    os.makedirs(current_path + 'checkpoints')"
   ],
   "id": "de82b716-f343-42a6-97b8-ba88169aa8fc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implementation Code"
   ],
   "id": "50513e83-3c6f-494b-b63b-f1bd103f32b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 ResNet Code"
   ],
   "id": "e21248ac-799d-4f97-ac7a-5226491d4bbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "# From https://github.com/uoguelph-mlrg/Cutout/blob/master/model/resnet.py\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes)"
   ],
   "id": "612125c2-0068-42bc-9b6f-038c5b1c1124"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Model Evaluate Test Code\n",
    "\n",
    "This function evaluates the performance of the model on a given data loader (loader). It sets the model to evaluation mode (eval), calculates the accuracy on the dataset, and returns the validation accuracy. It then switches the model back to training mode (train) before returning the validation accuracy."
   ],
   "id": "4cc5c7bb-c367-419f-af55-55a212d4375e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, cnn):\n",
    "    cnn.eval()    # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for images, labels in loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = cnn(images)\n",
    "\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "    val_acc = correct / total\n",
    "    cnn.train()\n",
    "    return val_acc"
   ],
   "id": "d5444a04-d092-4191-9794-c7954d8577b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training ResNet-18 in CIFAR-10"
   ],
   "id": "0426f4c4-9cea-4e5b-964d-e59644a3bd9f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Training ResNet-18 in CF10 without Cutout"
   ],
   "id": "f91716eb-de74-4e29-96e7-71929a71a477"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10"
   ],
   "id": "d286c46a-561b-475d-a250-bcd3583cb923"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10 = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar10.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "ae895276-3efa-4684-86d4-093a2a8d0556"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10"
   ],
   "id": "d04291cc-0bc7-4bb5-b5b1-6d8f79700f21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "86bf3a64-8f7c-4820-abf1-be13b781c282"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "2c431a9b-8204-46c4-a590-479198fe42d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10 = 128\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(dataset=train_dataset_cifar10,\n",
    "                                           batch_size=batch_size_cifar10,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "58a8de9b-067f-4c72-8b75-54cbe09c7182"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "759df98e-18c0-4178-be37-08517db51066"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "8c64ae8e-0ded-44ca-8167-f74d664dc264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar10 = \"resnet18_cifar10\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "resnet18_cifar10 = ResNet18(num_classes=num_classes_cifar10)\n",
    "\n",
    "\n",
    "resnet18_cifar10 = resnet18_cifar10.cuda()\n",
    "learning_rate_resnet18_cifar10 = 0.1\n",
    "criterion_resnet18_cifar10 = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar10 = torch.optim.SGD(resnet18_cifar10.parameters(), lr=learning_rate_resnet18_cifar10,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar10 = MultiStepLR(cnn_optimizer_resnet18_cifar10, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "3765edbf-4e89-4741-b1fb-30094c7b3f2f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 withuout Cutout"
   ],
   "id": "ab9ad1b9-e0cf-4847-a5eb-aa955b0d1086"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "8da359dc-8d42-47db-9c44-a1b256a3109b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar10.zero_grad()\n",
    "        pred = resnet18_cifar10(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar10(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar10.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_accr_resnet18_cifar10 = test(test_loader_cifar10, resnet18_cifar10)\n",
    "    tqdm.write('test_acc: %.3f' % (test_accr_resnet18_cifar10))\n",
    "\n",
    "    scheduler_resnet18_cifar10.step()    \n",
    "\n",
    "    \n",
    "torch.save(resnet18_cifar10.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar10 + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar10 = (1 - test(test_loader_cifar10, resnet18_cifar10))*100\n",
    "print('Final Result ResNet-18 without Cutout for Test CIFAR-10 Dataset: %.3f' % (final_test_acc_resnet18_cifar10))"
   ],
   "id": "30461e44-5653-4273-9785-129cc8dbd5de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Training ResNet-18 in CF10 with Cutout"
   ],
   "id": "e1db242b-f2d4-43d8-8536-cd8f41d1b646"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutout Code"
   ],
   "id": "a0abc0af-f6fc-47eb-9378-71e6cd9698b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: link to the file in the original repo that this comes from\n",
    "# Source Code from https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ],
   "id": "d7284255-6b20-4936-a263-4d33e374c0b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10"
   ],
   "id": "4999e27d-656b-4510-8e56-55162dda3573"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_cutout = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar10_cutout.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_cutout.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar10 = 1\n",
    "length_cifar10 = 16\n",
    "train_transform_cifar10_cutout.transforms.append(Cutout(n_holes=n_holes_cifar10, length=length_cifar10))\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "72d7c3e0-2868-443c-aafd-844911b7569a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10"
   ],
   "id": "cc3f51f6-5aa1-47c5-97ab-58fd0f00aabc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_cutout = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_cutout,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "6ac7c064-ce5e-4f56-8208-e1b8e03aeb58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "e0b4a27d-8bca-4d70-966b-2cebccf9a390"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_cutout = 128\n",
    "train_loader_cifar10_cutout = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_cutout,\n",
    "                                           batch_size=batch_size_cifar10_cutout,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_cutout,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "66000b67-ccba-4879-bdae-dc5c88c56fef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "e59c412b-4a21-45e1-8b8a-93a5fb2e5994"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "cbfc8205-39e3-46a2-9ae8-c0f6e513db95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar10_cutout = \"resnet18_cifar10_cutout\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "resnet18_cifar10_cutout = ResNet18(num_classes=num_classes_cifar10)\n",
    "\n",
    "\n",
    "resnet18_cifar10_cutout = resnet18_cifar10_cutout.cuda()\n",
    "learning_rate_resnet18_cifar10_cutout = 0.1\n",
    "criterion_resnet18_cifar10_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar10_cutout = torch.optim.SGD(resnet18_cifar10_cutout.parameters(), lr=learning_rate_resnet18_cifar10_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar10_cutout = MultiStepLR(cnn_optimizer_resnet18_cifar10_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "70eea595-e702-460c-901b-0d327e5c6f76"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Cutout"
   ],
   "id": "a2078e77-425d-4ed7-9d73-4278103db6c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "d99dcf62-f513-4516-908b-735f7e880b82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_cutout)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar10_cutout.zero_grad()\n",
    "        pred = resnet18_cifar10_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar10_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar10_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar10 = test(test_loader_cifar10,resnet18_cifar10_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar10))\n",
    "    scheduler_resnet18_cifar10_cutout.step()     \n",
    "torch.save(resnet18_cifar10_cutout.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar10_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar10_cutout = (1 - test(test_loader_cifar10,resnet18_cifar10_cutout))*100\n",
    "print('Final Result ResNet-18 using Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_cutout))"
   ],
   "id": "74f7ac3d-e02e-45f8-8d00-c0fb463adfbc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Training ResNet-18 in CF10 with Data Augmentation"
   ],
   "id": "f8c5b8e0-3f8b-4a3c-9b2b-e6b4ed77a81c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10"
   ],
   "id": "a88f4347-ffed-4120-9e86-28aca2e2e84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_da = transforms.Compose([])\n",
    "train_transform_cifar10_da.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar10_da.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar10_da.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_da.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "7dfbd2be-de09-4f02-ba18-28304e32b6f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10"
   ],
   "id": "56572fc1-7989-4b38-9e04-ff8496670afa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_da = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_da,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "9863221c-2b4e-4ee5-8694-3764759d4954"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "137eea0c-09be-45e5-841e-8e2253548b98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_da = 128\n",
    "train_loader_cifar10_da = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_da,\n",
    "                                           batch_size=batch_size_cifar10_da,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_da,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "f20ad27e-4ba0-47dc-b536-753c33ed1b13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "59a7ded7-3994-40e8-8d32-1040dc5048d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "2756fc2c-03c2-4ee3-b99b-bd31f70a50b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar10_da = \"resnet18_cifar10_da\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "resnet18_cifar10_da = ResNet18(num_classes=num_classes_cifar10)\n",
    "\n",
    "\n",
    "resnet18_cifar10_da = resnet18_cifar10_da.cuda()\n",
    "learning_rate_resnet18_cifar10_da = 0.1\n",
    "criterion_resnet18_cifar10_da = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar10_da = torch.optim.SGD(resnet18_cifar10_da.parameters(), lr=learning_rate_resnet18_cifar10_da,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar10_da = MultiStepLR(cnn_optimizer_resnet18_cifar10_da, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "b3fbe5ea-7da9-46a6-9218-e5342ea88608"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Data Augmentation"
   ],
   "id": "d2b9d0f4-60a3-4b98-b6cc-93ed3cf7c5e6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "88b65ebf-9816-4c89-9a5a-0f8cbedfe030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_da)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar10_da.zero_grad()\n",
    "        pred = resnet18_cifar10_da(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar10_da(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar10_da.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_resnet18_cifar10_da = test(test_loader_cifar10,resnet18_cifar10_da)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_resnet18_cifar10_da))\n",
    "    scheduler_resnet18_cifar10_da.step()     \n",
    "torch.save(resnet18_cifar10_da.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar10_da + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar10_da = (1 - test(test_loader_cifar10,resnet18_cifar10_da))*100\n",
    "print('Final Result ResNet-18 using Data Augmentation for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_da))"
   ],
   "id": "1e019731-2c2c-42f4-a692-74ddde39f728"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Training ResNet-18 in CF10 with Data Augmentation with Cutout"
   ],
   "id": "2376f7b4-1f81-4bb5-a738-d92548241b74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-10"
   ],
   "id": "71320619-f42b-4cbd-9141-dc851eb1c183"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar10 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar10_da_co = transforms.Compose([])\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar10_da_co.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar10_da_co.transforms.append(normalize_image_cifar10)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar10_da_co = 1\n",
    "length_cifar10_da_co = 16\n",
    "train_transform_cifar10_da_co.transforms.append(Cutout(n_holes=n_holes_cifar10_da_co, length=length_cifar10_da_co))\n",
    "\n",
    "\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar10])"
   ],
   "id": "57153188-e51d-40bd-999d-10f0e0e3ecb1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-10"
   ],
   "id": "64f2f8d8-224b-4242-895f-68611ecf40a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar10_da_co = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar10_da_co,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar10,\n",
    "                                    download=True)"
   ],
   "id": "868ee30e-6a12-4435-9da5-da5811d7f57b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "90a56d6b-2069-411b-9fd4-1332598f8311"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar10_da_co = 128\n",
    "train_loader_cifar10_da_co = torch.utils.data.DataLoader(dataset=train_dataset_cifar10_da_co,\n",
    "                                           batch_size=batch_size_cifar10_da_co,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10,\n",
    "                                          batch_size=batch_size_cifar10_da_co,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "06377dbd-19a6-46d7-85a0-6312d4db878e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "63d5b943-d03f-4f3a-be87-43e388574489"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "b723e440-6389-425f-8c52-b6171bdd8a80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar10_da_cutout = \"resnet18_cifar10_da_cutout\"\n",
    "\n",
    "num_classes_cifar10 = 10\n",
    "resnet18_cifar10_da_cutout = ResNet18(num_classes=num_classes_cifar10)\n",
    "\n",
    "\n",
    "resnet18_cifar10_da_cutout = resnet18_cifar10_da_cutout.cuda()\n",
    "learning_rate_cifar10_da_cutout = 0.1\n",
    "criterion_cifar10_da_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_cifar10_da_cutout = torch.optim.SGD(resnet18_cifar10_da_cutout.parameters(), lr=learning_rate_cifar10_da_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_cifar10_da_cutout = MultiStepLR(cnn_optimizer_cifar10_da_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "b1431ee4-8d1f-4c2b-9f99-cc0a566de652"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Cutout"
   ],
   "id": "f1cc99a9-ab74-4da3-98de-10c7b7cb741e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "9484d1cc-6edd-4e09-93e8-b03d8c7f9e08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar10_da_co)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar10_da_cutout.zero_grad()\n",
    "        pred = resnet18_cifar10_da_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_cifar10_da_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_cifar10_da_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar10_da_cutout = test(test_loader_cifar10,resnet18_cifar10_da_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar10_da_cutout))\n",
    "    scheduler_cifar10_da_cutout.step()     \n",
    "torch.save(resnet18_cifar10_da_cutout.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar10_da_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar10_da_cutout = (1 - test(test_loader_cifar10,resnet18_cifar10_da_cutout))*100\n",
    "print('Final Result ResNet-18 using Data Augmentation and  Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_da_cutout))"
   ],
   "id": "d3bbbfc3-7708-4ccc-8009-4b518e0488be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Result ResNet-18 without Cutout for Test CIFAR-10 Dataset: %.3f' % (final_test_acc_resnet18_cifar10))\n",
    "print('Final Result ResNet-18 using Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_cutout))\n",
    "print('Final Result ResNet-18 using Data Augmentation for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_da))\n",
    "print('Final Result ResNet-18 using Data Augmentation and  Cutout for CIFAR-10 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar10_da_cutout))"
   ],
   "id": "fd959d5d-bb36-43d4-b20f-39387530ff2d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training ResNet-18 in CIFAR-100"
   ],
   "id": "bc964cf6-b288-4b4f-b16f-8547354dc0d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Training ResNet-18 in CF100 without Cutout"
   ],
   "id": "fb8878be-8841-4a81-a85e-d9e57cdaacdf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "19dbec91-7fde-482f-bd0b-8fcf85b19263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100 = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar100.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "da85636b-0f38-4356-8a50-a34b3d571d30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "28871949-f52a-4a76-9522-948623034ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "1fff388f-2039-4ef2-b566-a1dcca0ab4b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "62eedea9-2f52-4db0-82b8-ed1f3e60416f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100 = 128\n",
    "train_loader_cifar100 = torch.utils.data.DataLoader(dataset=train_dataset_cifar100,\n",
    "                                           batch_size=batch_size_cifar100,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "3c0e66e7-bb83-4aec-8420-e5abded86a41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "dc2eefdf-69b7-457b-9a63-314f397bb0d1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "a2cab5f3-a934-42cd-84d4-26b8ea4c9f57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar100 = \"resnet18_cifar100\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "resnet18_cifar100 = ResNet18(num_classes=num_classes_cifar100)\n",
    "\n",
    "\n",
    "resnet18_cifar100 = resnet18_cifar100.cuda()\n",
    "learning_rate_resnet18_cifar100 = 0.1\n",
    "criterion_resnet18_cifar100 = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar100 = torch.optim.SGD(resnet18_cifar100.parameters(), lr=learning_rate_resnet18_cifar100,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar100 = MultiStepLR(cnn_optimizer_resnet18_cifar100, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "0be9082f-aa9b-4152-b22d-83874ec6179c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 withuout Cutout"
   ],
   "id": "1a57cf52-a94f-4c97-ad23-f76dc2237665"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "fcb0a9e8-e3ad-4a7e-87c3-b7a6c642fb3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar100.zero_grad()\n",
    "        pred = resnet18_cifar100(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar100(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar100.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_accr_resnet18_cifar100 = test(test_loader_cifar100, resnet18_cifar100)\n",
    "    tqdm.write('test_acc: %.3f' % (test_accr_resnet18_cifar100))\n",
    "\n",
    "    scheduler_resnet18_cifar100.step()   \n",
    "    \n",
    "torch.save(resnet18_cifar100.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar100 + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar100 = (1 - test(test_loader_cifar100, resnet18_cifar100))*100\n",
    "print('Final Result ResNet-18 without Cutout for Test CIFAR-100 Dataset: %.3f' % (final_test_acc_resnet18_cifar100))"
   ],
   "id": "113fb05e-3c65-4620-a22b-0479518f456c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Training ResNet-18 in CF100 with Cutout"
   ],
   "id": "a1262ce7-c383-40d1-99f9-5bb48e5c90f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "fc089c5a-b4a3-47b4-845f-0083be52cd9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_cutout = transforms.Compose([])\n",
    "\n",
    "train_transform_cifar100_cutout.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_cutout.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar100 = 1\n",
    "length_cifar100 = 8\n",
    "train_transform_cifar100_cutout.transforms.append(Cutout(n_holes=n_holes_cifar100, length=length_cifar100))\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "eb453dc7-eb1b-4bb5-9993-0d62dd4be1ed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-0"
   ],
   "id": "414668f0-9425-4055-ab82-93c38ffd18af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_cutout = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_cutout,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "a23cf8fd-3dfa-45d0-978e-8f07919e5eda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "fced9f6c-700d-40a2-b996-c686c247ea3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_cutout = 128\n",
    "train_loader_cifar100_cutout = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_cutout,\n",
    "                                           batch_size=batch_size_cifar100_cutout,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_cutout,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "f6ba179d-c9b5-49d9-81d4-1e75418bee14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "740c0842-4e32-4cbc-95b6-3a1970be0a34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "331cff12-71bc-4969-99e5-1699df1280f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar100_cutout = \"resnet18_cifar100_cutout\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "resnet18_cifar100_cutout = ResNet18(num_classes=num_classes_cifar100)\n",
    "\n",
    "\n",
    "resnet18_cifar100_cutout = resnet18_cifar100_cutout.cuda()\n",
    "learning_rate_resnet18_cifar100_cutout = 0.1\n",
    "criterion_resnet18_cifar100_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar100_cutout = torch.optim.SGD(resnet18_cifar100_cutout.parameters(), lr=learning_rate_resnet18_cifar100_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar100_cutout = MultiStepLR(cnn_optimizer_resnet18_cifar100_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "9667e00d-3065-4627-bc46-e49ebe928c17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Cutout"
   ],
   "id": "d642d95e-cb50-4db2-84e6-d82b7efa8b78"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "85056c32-7a86-4a22-a46c-31618f856bf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_cutout)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar100_cutout.zero_grad()\n",
    "        pred = resnet18_cifar100_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar100_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar100_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar100 = test(test_loader_cifar100,resnet18_cifar100_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar100))\n",
    "    scheduler_resnet18_cifar100_cutout.step()     \n",
    "torch.save(resnet18_cifar100_cutout.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar100_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar100_cutout = (1 - test(test_loader_cifar100,resnet18_cifar100_cutout))*100\n",
    "print('Final Result ResNet-18 using Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_cutout))"
   ],
   "id": "8d3710a1-42c6-44b0-9eef-307335c2d256"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Training ResNet-18 in CF100 with Data Augmentation"
   ],
   "id": "627a571c-fca0-4dec-a406-de7de7b06bf0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "8b420ad6-26da-414f-9904-44ab27433d59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_da = transforms.Compose([])\n",
    "train_transform_cifar100_da.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar100_da.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar100_da.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_da.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "645aa2cb-fa45-41e7-ab76-898e6a69979c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "57d778c0-9054-47ee-8927-54e7cadec5da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_da = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_da,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "9638d477-f9e7-4e3e-8e39-7b6e27d7542f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "870d2304-60b4-4607-9f34-166508bf49c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_da = 128\n",
    "train_loader_cifar100_da = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_da,\n",
    "                                           batch_size=batch_size_cifar100_da,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_da,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "42e38c0b-4608-4572-8a93-22c9b844f186"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "211ba6b4-bdee-44b4-96a6-bd744a535e44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "6fd9dfe8-8201-4d9a-bcaa-1f17618f8003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar100_da = \"resnet18_cifar100_da\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "resnet18_cifar100_da = ResNet18(num_classes=num_classes_cifar100)\n",
    "\n",
    "\n",
    "resnet18_cifar100_da = resnet18_cifar100_da.cuda()\n",
    "learning_rate_resnet18_cifar100_da = 0.1\n",
    "criterion_resnet18_cifar100_da = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_resnet18_cifar100_da = torch.optim.SGD(resnet18_cifar100_da.parameters(), lr=learning_rate_resnet18_cifar100_da,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_resnet18_cifar100_da = MultiStepLR(cnn_optimizer_resnet18_cifar100_da, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "3da616bd-698f-410e-a52f-602174c4af1e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Data Augmentation"
   ],
   "id": "6a637a57-55fc-4af9-98e3-c485ae6ad6b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "156d9887-3e22-4ff9-8e96-53dedfad8fa1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_da)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar100_da.zero_grad()\n",
    "        pred = resnet18_cifar100_da(images)\n",
    "\n",
    "        xentropy_loss = criterion_resnet18_cifar100_da(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_resnet18_cifar100_da.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_resnet18_cifar100_da = test(test_loader_cifar100,resnet18_cifar100_da)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_resnet18_cifar100_da))\n",
    "    scheduler_resnet18_cifar100_da.step()     \n",
    "torch.save(resnet18_cifar100_da.state_dict(), current_path + 'checkpoints/' + file_name_resnet18_cifar100_da + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar100_da = (1 - test(test_loader_cifar100,resnet18_cifar100_da))*100\n",
    "print('Final Result ResNet-18 using Data Augmentation for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_da))"
   ],
   "id": "cefdaf07-702a-49aa-a09a-5d1338af3b00"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Training ResNet-18 in CF100 with Data Augmentation with Cutout"
   ],
   "id": "c5d28a45-429d-4e9f-a86f-320980202431"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Processing for CIFAR-100"
   ],
   "id": "186bf311-e4b8-4475-b010-c645e8bc27ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "normalize_image_cifar100 = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]], std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "train_transform_cifar100_da_co = transforms.Compose([])\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_cifar100_da_co.transforms.append(transforms.ToTensor())\n",
    "train_transform_cifar100_da_co.transforms.append(normalize_image_cifar100)\n",
    "\n",
    "#Add Cutout to the image transformer pipeline\n",
    "n_holes_cifar100_da_co = 1\n",
    "length_cifar100_da_co = 8\n",
    "train_transform_cifar100_da_co.transforms.append(Cutout(n_holes=n_holes_cifar100_da_co, length=length_cifar100_da_co))\n",
    "\n",
    "\n",
    "test_transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_image_cifar100])"
   ],
   "id": "0e7280c5-1705-4080-8599-4d76ae1ed462"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset of CIFAR-100"
   ],
   "id": "e8f0b13f-146b-46b4-aea0-5012f3674ef5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar100_da_co = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                     train=True,\n",
    "                                     transform=train_transform_cifar100_da_co,\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset_cifar100 = datasets.CIFAR100(root=current_path + 'data/',\n",
    "                                    train=False,\n",
    "                                    transform=test_transform_cifar100,\n",
    "                                    download=True)"
   ],
   "id": "ff1a0436-56c3-4ee1-95da-6196af8aa330"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset as Dataloader"
   ],
   "id": "f92164b5-d91f-4aba-99ad-8335ef55ebeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size_cifar100_da_co = 128\n",
    "train_loader_cifar100_da_co = torch.utils.data.DataLoader(dataset=train_dataset_cifar100_da_co,\n",
    "                                           batch_size=batch_size_cifar100_da_co,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(dataset=test_dataset_cifar100,\n",
    "                                          batch_size=batch_size_cifar100_da_co,\n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True,\n",
    "                                          num_workers=2)"
   ],
   "id": "03accc79-5735-4ad8-a409-b93460004615"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ],
   "id": "3a15822d-315b-485d-90eb-99fbf0244d2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up the machine learning model, loss function, optimizer, and learning rate scheduler."
   ],
   "id": "06884319-55c8-4e8a-aed8-1822c5741c9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name will be the used for the name of the file of weight of the model and also the result\n",
    "file_name_resnet18_cifar100_da_cutout = \"resnet18_cifar100_da_cutout\"\n",
    "\n",
    "num_classes_cifar100 = 100\n",
    "resnet18_cifar100_da_cutout = ResNet18(num_classes=num_classes_cifar100)\n",
    "\n",
    "\n",
    "resnet18_cifar100_da_cutout = resnet18_cifar100_da_cutout.cuda()\n",
    "learning_rate_cifar100_da_cutout = 0.1\n",
    "criterion_cifar100_da_cutout = nn.CrossEntropyLoss().cuda()\n",
    "cnn_optimizer_cifar100_da_cutout = torch.optim.SGD(resnet18_cifar100_da_cutout.parameters(), lr=learning_rate_cifar100_da_cutout,\n",
    "                                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler_cifar100_da_cutout = MultiStepLR(cnn_optimizer_cifar100_da_cutout, milestones=[60, 120, 160], gamma=0.2)"
   ],
   "id": "9ccb3075-28ba-44ee-acde-fd0f9ce311f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet-18 with Cutout"
   ],
   "id": "73981d82-1a2d-4957-92ba-02c4802b3b5a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the training loop for the chosen machine learning model over a specified number of epochs. Each epoch involves a forward pass, loss computation, backpropagation, and parameter updates. It also calculates and displays the training accuracy and cross-entropy loss. At the end of each epoch, the model’s performance is evaluated on the test set, and the results are logged and saved."
   ],
   "id": "36446870-eb29-467a-9794-25ca673ccc30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader_cifar100_da_co)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        resnet18_cifar100_da_cutout.zero_grad()\n",
    "        pred = resnet18_cifar100_da_cutout(images)\n",
    "\n",
    "        xentropy_loss = criterion_cifar100_da_cutout(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        cnn_optimizer_cifar100_da_cutout.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # Calculate running average of accuracy\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            xentropy='%.3f' % (xentropy_loss_avg / (i + 1)),\n",
    "            acc='%.3f' % accuracy)\n",
    "\n",
    "    test_acc_cifar100_da_cutout = test(test_loader_cifar100,resnet18_cifar100_da_cutout)\n",
    "    tqdm.write('test_acc: %.3f' % (test_acc_cifar100_da_cutout))\n",
    "    scheduler_cifar100_da_cutout.step()     \n",
    "torch.save(resnet18_cifar100_da_cutout.state_dict(),current_path +  'checkpoints/' + file_name_resnet18_cifar100_da_cutout + '.pt')\n",
    "\n",
    "\n",
    "final_test_acc_resnet18_cifar100_da_cutout = (1 - test(test_loader_cifar100,resnet18_cifar100_da_cutout))*100\n",
    "print('Final Result ResNet-18 using Data Augmentation and  Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_da_cutout))"
   ],
   "id": "af5f6bc1-309b-463a-b455-24df35c55846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Result ResNet-18 without Cutout for Test CIFAR-100 Dataset: %.3f' % (final_test_acc_resnet18_cifar100))\n",
    "print('Final Result ResNet-18 using Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_cutout))\n",
    "print('Final Result ResNet-18 using Data Augmentation for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_da))\n",
    "print('Final Result ResNet-18 using Data Augmentation and  Cutout for CIFAR-100 Test Dataset: %.3f' % (final_test_acc_resnet18_cifar100_da_cutout))"
   ],
   "id": "8c3f6ceb-726e-436f-8801-df7dffd9752d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
